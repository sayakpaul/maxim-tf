{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sayakpaul/maxim-tf/blob/main/notebooks/inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wxgb-sWfpb49"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "This notebook shows how to run inference with the [MAXIM family of models](https://github.com/google-research/maxim) from [TensorFlow Hub](https://tfhub.dev/sayakpaul/collections/maxim/1). MAXIM family of models share the same backbone for performing: denoising, dehazing, deblurring, deraining, and enhancement. You can know more about the public MAXIM models from [here](https://github.com/google-research/maxim#results-and-pre-trained-models)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zlp7twW3tB2n"
      },
      "source": [
        "## Select a checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-n8jA4Gojv2",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "task = \"Dehazing_Indoor\"  # @param [\"Denoising\", \"Dehazing_Indoor\", \"Dehazing_Outdoor\", \"Deblurring\", \"Deraining\", \"Enhancement\", \"Retouching\"]\n",
        "\n",
        "model_handle_map = {\n",
        "    \"Denoising\": [\n",
        "        \"https://tfhub.dev/sayakpaul/maxim_s-3_denoising_sidd/1\",\n",
        "        \"https://github.com/google-research/maxim/raw/main/maxim/images/Denoising/input/0003_30.png\",\n",
        "    ],\n",
        "    \"Dehazing_Indoor\": [\n",
        "        \"https://tfhub.dev/sayakpaul/maxim_s-2_dehazing_sots-indoor/1\",\n",
        "        \"https://github.com/google-research/maxim/raw/main/maxim/images/Dehazing/input/0003_0.8_0.2.png\",\n",
        "    ],\n",
        "    \"Dehazing_Outdoor\": [\n",
        "        \"https://tfhub.dev/sayakpaul/maxim_s-2_dehazing_sots-outdoor/1\",\n",
        "        \"https://github.com/google-research/maxim/raw/main/maxim/images/Dehazing/input/1444_10.png\",\n",
        "    ],\n",
        "    \"Deblurring\": [\n",
        "        \"https://tfhub.dev/sayakpaul/maxim_s-3_deblurring_gopro/1\",\n",
        "        \"https://github.com/google-research/maxim/raw/main/maxim/images/Deblurring/input/1fromGOPR0950.png\",\n",
        "    ],\n",
        "    \"Deraining\": [\n",
        "        \"https://tfhub.dev/sayakpaul/maxim_s-2_deraining_raindrop/1\",\n",
        "        \"https://github.com/google-research/maxim/raw/main/maxim/images/Deraining/input/15.png\",\n",
        "    ],\n",
        "    \"Enhancement\": [\n",
        "        \"https://tfhub.dev/sayakpaul/maxim_s-2_enhancement_lol/1\",\n",
        "        \"https://github.com/google-research/maxim/raw/main/maxim/images/Enhancement/input/a4541-DSC_0040-2.png\",\n",
        "    ],\n",
        "    \"Retouching\": [\n",
        "        \"https://tfhub.dev/sayakpaul/maxim_s-2_enhancement_fivek/1\",\n",
        "        \"https://github.com/google-research/maxim/raw/main/maxim/images/Enhancement/input/a4541-DSC_0040-2.png\",\n",
        "    ],\n",
        "}\n",
        "\n",
        "model_handle = model_handle_map[task]\n",
        "ckpt = model_handle[0]\n",
        "print(f\"TF-Hub handle: {ckpt}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3t6c3Z1Pz6UT"
      },
      "source": [
        "For deblurring, there are other checkpoints too:\n",
        "\n",
        "* https://tfhub.dev/sayakpaul/maxim_s-3_deblurring_realblur_r/1\n",
        "* https://tfhub.dev/sayakpaul/maxim_s-3_deblurring_realblur_j/1\n",
        "* https://tfhub.dev/sayakpaul/maxim_s-3_deblurring_reds/1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNlQ2HzrtJOU"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEhpgokqtKFz"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTdrCvUltkCn"
      },
      "source": [
        "## Fetch the input image based on the task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bn90H1rltRcM"
      },
      "outputs": [],
      "source": [
        "image_url = model_handle[1]\n",
        "image_path = tf.keras.utils.get_file(origin=image_url)\n",
        "Image.open(image_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsXRY1kvum4O"
      },
      "source": [
        "## Preprocessing utilities\n",
        "\n",
        "Based on [this official script](https://github.com/google-research/maxim/blob/main/maxim/run_eval.py)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGmgEtfLt7S5"
      },
      "outputs": [],
      "source": [
        "# Since the model was not initialized to take variable-length sizes (None, None, 3),\n",
        "# we need to be careful about how we are resizing the images.\n",
        "# From https://www.tensorflow.org/lite/examples/style_transfer/overview#pre-process_the_inputs\n",
        "def resize_image(image, target_dim):\n",
        "    # Resize the image so that the shorter dimension becomes `target_dim`.\n",
        "    shape = tf.cast(tf.shape(image)[1:-1], tf.float32)\n",
        "    short_dim = min(shape)\n",
        "    scale = target_dim / short_dim\n",
        "    new_shape = tf.cast(shape * scale, tf.int32)\n",
        "    image = tf.image.resize(image, new_shape)\n",
        "\n",
        "    # Central crop the image.\n",
        "    image = tf.image.resize_with_crop_or_pad(image, target_dim, target_dim)\n",
        "\n",
        "    return image\n",
        "\n",
        "\n",
        "def process_image(image_path, target_dim=256):\n",
        "    input_img = np.asarray(Image.open(image_path).convert(\"RGB\"), np.float32) / 255.0\n",
        "    input_img = tf.expand_dims(input_img, axis=0)\n",
        "    input_img = resize_image(input_img, target_dim)\n",
        "    return input_img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxsxGhvKvDrF"
      },
      "source": [
        "This notebook infers on fixed-shape images. However, MAXIM can handle images of any resolution. The current implementation in TensorFlow can achieve this with a bit of hacking. Please refer to [this notebook](https://github.com/sayakpaul/maxim-tf/blob/main/notebooks/inference-dynamic-resize.ipynb) if you want the model to infer on dynamic shapes. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5T20A1xLvcLq"
      },
      "source": [
        "## Run predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOxESHl2vdRE"
      },
      "outputs": [],
      "source": [
        "def get_model(model_url: str, input_resolution: tuple) -> tf.keras.Model:\n",
        "    inputs = tf.keras.Input((*input_resolution, 3))\n",
        "    hub_module = hub.KerasLayer(model_url)\n",
        "\n",
        "    outputs = hub_module(inputs)\n",
        "\n",
        "    return tf.keras.Model(inputs, outputs)\n",
        "\n",
        "\n",
        "# Based on https://github.com/google-research/maxim/blob/main/maxim/run_eval.py\n",
        "def infer(image_path: str, model: tf.keras.Model, input_resolution=(256, 256)):\n",
        "    preprocessed_image = process_image(image_path, input_resolution[0])\n",
        "\n",
        "    preds = model.predict(preprocessed_image)\n",
        "    if isinstance(preds, list):\n",
        "        preds = preds[-1]\n",
        "        if isinstance(preds, list):\n",
        "            preds = preds[-1]\n",
        "\n",
        "    preds = np.array(preds[0], np.float32)\n",
        "    final_pred_image = np.array((np.clip(preds, 0.0, 1.0)).astype(np.float32))\n",
        "    return final_pred_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Fr-rYLpwab6"
      },
      "outputs": [],
      "source": [
        "input_resolution = (256, 256)\n",
        "\n",
        "model = get_model(ckpt, input_resolution)\n",
        "\n",
        "final_pred_image = infer(image_path, model, input_resolution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTq1J42tw67G"
      },
      "source": [
        "## Visualize results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECGdFWQBw8E2"
      },
      "outputs": [],
      "source": [
        "# Based on https://www.tensorflow.org/lite/examples/style_transfer/overview#visualize_the_inputs\n",
        "def imshow(image, title=None):\n",
        "    if len(image.shape) > 3:\n",
        "        image = tf.squeeze(image, axis=0)\n",
        "\n",
        "    plt.imshow(image)\n",
        "    if title:\n",
        "        plt.title(title)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(15, 15))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "input_image = np.asarray(Image.open(image_path).convert(\"RGB\"), np.float32) / 255.0\n",
        "imshow(input_image, \"Input Image\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "imshow(final_pred_image, \"Predicted Image\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}