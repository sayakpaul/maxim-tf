{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wxgb-sWfpb49"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook shows how to run inference with the [MAXIM family of models](https://github.com/google-research/maxim) from [TensorFlow Hub] (TODO). MAXIM family of models share the same backbone for performing: denoising, dehazing, deblurring, deraining, and enhancement. You can know more about the public MAXIM models from [here](https://github.com/google-research/maxim#results-and-pre-trained-models)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zlp7twW3tB2n"
   },
   "source": [
    "## Select a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E-n8jA4Gojv2"
   },
   "outputs": [],
   "source": [
    "task = \"Dehazing_Indoor\"  # @param [\"Denoising\", \"Dehazing_Indoor\", \"Dehazing_Outdoor\", \"Deblurring\", \"Deraining\", \"Enhancement\", \"Retouching\"]\n",
    "\n",
    "model_handle_map = {\n",
    "    \"Denoising\": [\n",
    "        \"gs://maxim-tf/S-3_denoising_sidd\",\n",
    "        \"https://github.com/google-research/maxim/raw/main/maxim/images/Denoising/input/0003_30.png\",\n",
    "    ],\n",
    "    \"Dehazing_Indoor\": [\n",
    "        \"gs://maxim-tf/S-2_dehazing_sots-indoor\",\n",
    "        \"https://github.com/google-research/maxim/raw/main/maxim/images/Dehazing/input/0003_0.8_0.2.png\",\n",
    "    ],\n",
    "    \"Dehazing_Outdoor\": [\n",
    "        \"gs://maxim-tf/S-2_dehazing_sots-outdoor\",\n",
    "        \"https://github.com/google-research/maxim/raw/main/maxim/images/Dehazing/input/1444_10.png\",\n",
    "    ],\n",
    "    \"Deblurring\": [\n",
    "        \"gs://maxim-tf/S-3_deblurring_gopro\",\n",
    "        \"https://github.com/google-research/maxim/raw/main/maxim/images/Deblurring/input/1fromGOPR0950.png\",\n",
    "    ],\n",
    "    \"Deraining\": [\n",
    "        \"gs://maxim-tf/S-2_deraining_raindrop\",\n",
    "        \"https://github.com/google-research/maxim/raw/main/maxim/images/Deraining/input/15.png\",\n",
    "    ],\n",
    "    \"Enhancement\": [\n",
    "        \"gs://maxim-tf/S-2_enhancement_lol\",\n",
    "        \"https://github.com/google-research/maxim/raw/main/maxim/images/Enhancement/input/a4541-DSC_0040-2.png\",\n",
    "    ],\n",
    "    \"Retouching\": [\n",
    "        \"gs://maxim-tf/S-2_enhancement_fivek\",\n",
    "        \"https://github.com/google-research/maxim/raw/main/maxim/images/Enhancement/input/a4541-DSC_0040-2.png\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "model_handle = model_handle_map[task]\n",
    "ckpt = model_handle[0]\n",
    "print(f\"TF-Hub handle: {ckpt}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3t6c3Z1Pz6UT"
   },
   "source": [
    "For deblurring, there are other checkpoints too:\n",
    "\n",
    "* Item 1\n",
    "* Item 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SNlQ2HzrtJOU"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IEhpgokqtKFz"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UTdrCvUltkCn"
   },
   "source": [
    "## Fetch the input image based on the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bn90H1rltRcM"
   },
   "outputs": [],
   "source": [
    "image_url = model_handle[1]\n",
    "image_path = tf.keras.utils.get_file(origin=image_url)\n",
    "Image.open(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UsXRY1kvum4O"
   },
   "source": [
    "## Preprocessing utilities\n",
    "\n",
    "Based on [this official script](https://github.com/google-research/maxim/blob/main/maxim/run_eval.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZGmgEtfLt7S5"
   },
   "outputs": [],
   "source": [
    "# Since the model was not initialized to take variable-length sizes (None, None, 3),\n",
    "# we need to be careful about how we are resizing the images.\n",
    "# From https://www.tensorflow.org/lite/examples/style_transfer/overview#pre-process_the_inputs\n",
    "def resize_image(image, target_dim):\n",
    "    # Resize the image so that the shorter dimension becomes `target_dim`.\n",
    "    shape = tf.cast(tf.shape(image)[1:-1], tf.float32)\n",
    "    short_dim = min(shape)\n",
    "    scale = target_dim / short_dim\n",
    "    new_shape = tf.cast(shape * scale, tf.int32)\n",
    "    image = tf.image.resize(image, new_shape)\n",
    "\n",
    "    # Central crop the image.\n",
    "    image = tf.image.resize_with_crop_or_pad(image, target_dim, target_dim)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def process_image(image_path, target_dim=256):\n",
    "    input_img = np.asarray(Image.open(image_path).convert(\"RGB\"), np.float32) / 255.0\n",
    "    input_img = tf.expand_dims(input_img, axis=0)\n",
    "    input_img = resize_image(input_img, target_dim)\n",
    "    return input_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FxsxGhvKvDrF"
   },
   "source": [
    "This notebook infers on fixed-shape images. However, MAXIM can handle images of any resolution. The current implementation in TensorFlow can achieve this with a bit of hacking. Please refer to this notebook [TODO] if you want the model to infer on dynamic shapes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5T20A1xLvcLq"
   },
   "source": [
    "## Run predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hOxESHl2vdRE"
   },
   "outputs": [],
   "source": [
    "def get_model(model_url: str, input_resolution: tuple) -> tf.keras.Model:\n",
    "    inputs = tf.keras.Input((*input_resolution, 3))\n",
    "    hub_module = hub.KerasLayer(model_url)\n",
    "\n",
    "    outputs = hub_module(inputs)\n",
    "\n",
    "    return tf.keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "# Based on https://github.com/google-research/maxim/blob/main/maxim/run_eval.py\n",
    "def infer(image_path: str, model: tf.keras.Model, input_resolution=(256, 256)):\n",
    "    preprocessed_image = process_image(image_path, input_resolution[0])\n",
    "\n",
    "    preds = model.predict(preprocessed_image)\n",
    "    if isinstance(preds, list):\n",
    "        preds = preds[-1]\n",
    "        if isinstance(preds, list):\n",
    "            preds = preds[-1]\n",
    "\n",
    "    preds = np.array(preds[0], np.float32)\n",
    "    final_pred_image = np.array((np.clip(preds, 0.0, 1.0)).astype(np.float32))\n",
    "    return final_pred_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Fr-rYLpwab6"
   },
   "outputs": [],
   "source": [
    "input_resolution = (256, 256)\n",
    "\n",
    "model = get_model(ckpt, input_resolution)\n",
    "\n",
    "final_pred_image = infer(image_path, model, input_resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GTq1J42tw67G"
   },
   "source": [
    "## Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ECGdFWQBw8E2"
   },
   "outputs": [],
   "source": [
    "# Based on https://www.tensorflow.org/lite/examples/style_transfer/overview#visualize_the_inputs\n",
    "def imshow(image, title=None):\n",
    "    if len(image.shape) > 3:\n",
    "        image = tf.squeeze(image, axis=0)\n",
    "\n",
    "    plt.imshow(image)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "input_image = np.asarray(Image.open(image_path).convert(\"RGB\"), np.float32) / 255.0\n",
    "imshow(input_image, \"Input Image\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "imshow(final_pred_image, \"Predicted Image\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
