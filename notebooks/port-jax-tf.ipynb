{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f00a3bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b29530cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-09 13:18:28.774098: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from create_maxim_model import Model\n",
    "from maxim.configs import MAXIM_CONFIGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f817755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/google-research/maxim/blob/main/maxim/run_eval.py#L55\n",
    "\n",
    "VARIANT = \"S-3\"\n",
    "configs = MAXIM_CONFIGS.get(VARIANT)\n",
    "\n",
    "configs.update(\n",
    "    {\n",
    "        \"variant\": VARIANT,\n",
    "        \"dropout_rate\": 0.0,\n",
    "        \"num_outputs\": 3,\n",
    "        \"use_bias\": True,\n",
    "        \"num_supervision_scales\": 3,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "767355f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/google-research/maxim/blob/main/maxim/run_eval.py\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections \n",
    "import re\n",
    "import io\n",
    "\n",
    "tf.keras.utils.set_random_seed(2022)\n",
    "\n",
    "\n",
    "def recover_tree(keys, values):\n",
    "    \"\"\"Recovers a tree as a nested dict from flat names and values.\n",
    "    This function is useful to analyze checkpoints that are saved by our programs\n",
    "    without need to access the exact source code of the experiment. In particular,\n",
    "    it can be used to extract an reuse various subtrees of the scheckpoint, e.g.\n",
    "    subtree of parameters.\n",
    "    Args:\n",
    "      keys: a list of keys, where '/' is used as separator between nodes.\n",
    "      values: a list of leaf values.\n",
    "    Returns:\n",
    "      A nested tree-like dict.\n",
    "    \"\"\"\n",
    "    tree = {}\n",
    "    sub_trees = collections.defaultdict(list)\n",
    "    for k, v in zip(keys, values):\n",
    "        if \"/\" not in k:\n",
    "            tree[k] = v\n",
    "        else:\n",
    "            k_left, k_right = k.split(\"/\", 1)\n",
    "            sub_trees[k_left].append((k_right, v))\n",
    "    for k, kv_pairs in sub_trees.items():\n",
    "        k_subtree, v_subtree = zip(*kv_pairs)\n",
    "        tree[k] = recover_tree(k_subtree, v_subtree)\n",
    "    return tree\n",
    "\n",
    "\n",
    "def get_params(ckpt_path):\n",
    "    \"\"\"Get params checkpoint.\"\"\"\n",
    "\n",
    "    with tf.io.gfile.GFile(ckpt_path, \"rb\") as f:\n",
    "        data = f.read()\n",
    "    values = np.load(io.BytesIO(data))\n",
    "    params = recover_tree(*zip(*values.items()))\n",
    "    params = params[\"opt\"][\"target\"]\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401bcfdb",
   "metadata": {},
   "source": [
    "```\n",
    "gamma (TF) => scale (JAX)\n",
    "beta (TF) => bias (JAX)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f548473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/5491913/sorting-list-in-python\n",
    "def sort_nicely(l): \n",
    "    \"\"\" Sort the given iterable in the way that humans expect.\"\"\" \n",
    "    convert = lambda text: int(text) if text.isdigit() else text \n",
    "    alphanum_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ] \n",
    "    return sorted(l, key = alphanum_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cd7c488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_upsample(jax_params):\n",
    "    modified_jax_params = collections.OrderedDict()\n",
    "\n",
    "    jax_keys = list(jax_params.keys())\n",
    "    keys_upsampling = []\n",
    "    for k in range(len(jax_keys)):\n",
    "        if \"UpSample\" in jax_keys[k]:\n",
    "            keys_upsampling.append(jax_keys[k])\n",
    "    sorted_keys_upsampling = sort_nicely(keys_upsampling)\n",
    "    \n",
    "    i = 1\n",
    "    for k in sorted_keys_upsampling:\n",
    "        k_t = k.split(\"_\")[0] + \"_\" + str(i)\n",
    "        i += 1\n",
    "        for j in jax_params[k]:\n",
    "            for l in jax_params[k][j]:\n",
    "                modified_param_name = f\"{k_t}_{j}/{l}:0\"\n",
    "                params = jax_params[k][j][l]\n",
    "                modified_jax_params.update({modified_param_name: params})\n",
    "\n",
    "    return modified_jax_params\n",
    "\n",
    "\n",
    "def modify_jax_params(jax_params):\n",
    "    modified_jax_params = collections.OrderedDict()\n",
    "\n",
    "    for k in jax_params:\n",
    "        if \"UpSample\" not in k:\n",
    "            params = jax_params[k]\n",
    "\n",
    "            if (\"ConvTranspose\" in k) and (\"bias\" not in k):\n",
    "                params = params.transpose(0, 1, 3, 2)\n",
    "\n",
    "            split_names = k.split(\"_\")\n",
    "            modified_param_name = (\n",
    "                \"_\".join(split_names[0:-1]) + \"/\" + split_names[-1] + \":0\"\n",
    "            )\n",
    "\n",
    "            if \"layernorm\" in modified_param_name.lower():\n",
    "                if \"scale\" in modified_param_name:\n",
    "                    modified_param_name = modified_param_name.replace(\"scale\", \"gamma\")\n",
    "                elif \"bias\" in modified_param_name:\n",
    "                    modified_param_name = modified_param_name.replace(\"bias\", \"beta\")\n",
    "                \n",
    "            modified_jax_params.update({modified_param_name: params})\n",
    "\n",
    "    return modified_jax_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d59a2884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def port_jax_params(configs, ckpt_path):\n",
    "    # Initialize TF Model.\n",
    "    tf_model = Model(**configs)\n",
    "    \n",
    "    # Obtain a mapping of the TF variable names and their values.\n",
    "    tf_model_variables = tf_model.variables\n",
    "    tf_model_variables_dict = {}\n",
    "    for v in tf_model_variables:\n",
    "        tf_model_variables_dict[v.name] = v\n",
    "        \n",
    "    # Obtain the JAX pre-trained variables.\n",
    "    jax_params = get_params(ckpt_path)\n",
    "    [flat_jax_dict] = pd.json_normalize(jax_params, sep=\"_\").to_dict(orient=\"records\")\n",
    "    \n",
    "    # Amend the JAX variables to match the names of the TF variables.\n",
    "    modified_jax_params = modify_jax_params(flat_jax_dict)\n",
    "    modified_jax_params.update(modify_upsample(jax_params))\n",
    "    \n",
    "    # Porting.\n",
    "    tf_weights = []\n",
    "    i = 0\n",
    "\n",
    "    for k in modified_jax_params:\n",
    "        param = modified_jax_params[k]\n",
    "        tf_weights.append((tf_model_variables_dict[k], param))\n",
    "        i += 1\n",
    "\n",
    "    assert i == len(modified_jax_params) == len(tf_model_variables_dict)\n",
    "\n",
    "    tf.keras.backend.batch_set_value(tf_weights)\n",
    "\n",
    "    return modified_jax_params, tf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d27a99a4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-09 13:19:01.445333: W tensorflow/core/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"INVALID_ARGUMENT: Error executing an HTTP request: HTTP response code 400 with body '{\n",
      "  \"error\": \"invalid_grant\",\n",
      "  \"error_description\": \"Bad Request\"\n",
      "}'\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata\".\n"
     ]
    }
   ],
   "source": [
    "_, tf_model = port_jax_params(\n",
    "    configs, \"gs://gresearch/maxim/ckpt/Denoising/SIDD/checkpoint.npz\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12bf7702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 45s 45s/step\n"
     ]
    }
   ],
   "source": [
    "preds = tf_model.predict(np.ones((1, 256, 256, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fe13b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: ((256, 256, 3), array([[0.979627  , 0.9843435 , 0.97292805],\n",
      "       [1.0002174 , 0.99844694, 0.9940431 ],\n",
      "       [0.99668646, 0.9964776 , 0.9939418 ]], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "if isinstance(preds, list):\n",
    "    preds = preds[-1]\n",
    "    if isinstance(preds, list):\n",
    "        preds = preds[-1]\n",
    "        \n",
    "preds = np.array(preds[0], np.float32)\n",
    "print(f\"Predictions: {preds.shape, preds[0, :3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03f440a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model.save_weights(\"denoising_sidd.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4142a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.testing.assert_allclose(\n",
    "#     preds[0, :3],\n",
    "#     np.array(\n",
    "#         [\n",
    "#             [1.0001332, 1.0020351, 0.99739677],\n",
    "#             [0.999209, 1.0013864, 0.99625367],\n",
    "#             [1.0003445, 1.0004236, 0.996228],\n",
    "#         ]\n",
    "#     ),\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e3f41e",
   "metadata": {},
   "source": [
    "## Check if the params were successfully ported by running assertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d214ba96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the JAX pre-trained variables.\n",
    "ckpt_path = \"gs://gresearch/maxim/ckpt/Denoising/SIDD/checkpoint.npz\"\n",
    "jax_params = get_params(ckpt_path)\n",
    "[flat_jax_dict] = pd.json_normalize(jax_params, sep=\"_\").to_dict(orient=\"records\")\n",
    "\n",
    "# Amend the JAX variables to match the names of the TF variables.\n",
    "modified_jax_params = modify_jax_params(flat_jax_dict)\n",
    "modified_jax_params.update(modify_upsample(jax_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d79527ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_tf_params = tf_model.variables\n",
    "tf_model_variables_dict = {}\n",
    "for v in modified_tf_params:\n",
    "    tf_model_variables_dict[v.name] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58a523cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "unmatched_params = []\n",
    "\n",
    "for k in modified_jax_params:\n",
    "    jax_params = modified_jax_params[k]\n",
    "    tf_params = tf_model_variables_dict[k].numpy()\n",
    "\n",
    "    try:\n",
    "        np.testing.assert_allclose(jax_params, tf_params)\n",
    "    except:\n",
    "        unmatched_params.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b55cb05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmatched_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64cd93f",
   "metadata": {},
   "source": [
    "## JAX implementation of MAXIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e3b3f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"Main file for the MAXIM model.\"\"\"\n",
    "\n",
    "import functools\n",
    "from typing import Any, Sequence, Tuple\n",
    "\n",
    "import einops\n",
    "import flax.linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "Conv3x3 = functools.partial(nn.Conv, kernel_size=(3, 3))\n",
    "Conv1x1 = functools.partial(nn.Conv, kernel_size=(1, 1))\n",
    "ConvT_up = functools.partial(nn.ConvTranspose,\n",
    "                             kernel_size=(2, 2),\n",
    "                             strides=(2, 2))\n",
    "Conv_down = functools.partial(nn.Conv,\n",
    "                              kernel_size=(4, 4),\n",
    "                              strides=(2, 2))\n",
    "\n",
    "weight_initializer = nn.initializers.normal(stddev=2e-2)\n",
    "\n",
    "\n",
    "class MlpBlock(nn.Module):\n",
    "  \"\"\"A 1-hidden-layer MLP block, applied over the last dimension.\"\"\"\n",
    "  mlp_dim: int\n",
    "  dropout_rate: float = 0.0\n",
    "  use_bias: bool = True\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x, deterministic=True):\n",
    "    n, h, w, d = x.shape\n",
    "    x = nn.Dense(self.mlp_dim, use_bias=self.use_bias,\n",
    "                 kernel_init=weight_initializer)(x)\n",
    "    x = nn.gelu(x)\n",
    "    x = nn.Dropout(rate=self.dropout_rate)(x, deterministic)\n",
    "    x = nn.Dense(d, use_bias=self.use_bias,\n",
    "                 kernel_init=weight_initializer)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def block_images_einops(x, patch_size):\n",
    "  \"\"\"Image to patches.\"\"\"\n",
    "  batch, height, width, channels = x.shape\n",
    "  grid_height = height // patch_size[0]\n",
    "  grid_width = width // patch_size[1]\n",
    "  x = einops.rearrange(\n",
    "      x, \"n (gh fh) (gw fw) c -> n (gh gw) (fh fw) c\",\n",
    "      gh=grid_height, gw=grid_width, fh=patch_size[0], fw=patch_size[1])\n",
    "  return x\n",
    "\n",
    "\n",
    "def unblock_images_einops(x, grid_size, patch_size):\n",
    "  \"\"\"patches to images.\"\"\"\n",
    "  x = einops.rearrange(\n",
    "      x, \"n (gh gw) (fh fw) c -> n (gh fh) (gw fw) c\",\n",
    "      gh=grid_size[0], gw=grid_size[1], fh=patch_size[0], fw=patch_size[1])\n",
    "  return x\n",
    "\n",
    "\n",
    "class UpSampleRatio(nn.Module):\n",
    "  \"\"\"Upsample features given a ratio > 0.\"\"\"\n",
    "  features: int\n",
    "  ratio: float\n",
    "  use_bias: bool = True\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    n, h, w, c = x.shape\n",
    "    x = jax.image.resize(\n",
    "        x,\n",
    "        shape=(n, int(h * self.ratio), int(w * self.ratio), c),\n",
    "        method=\"bilinear\")\n",
    "    x = Conv1x1(features=self.features, use_bias=self.use_bias)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "class CALayer(nn.Module):\n",
    "  \"\"\"Squeeze-and-excitation block for channel attention.\n",
    "\n",
    "  ref: https://arxiv.org/abs/1709.01507\n",
    "  \"\"\"\n",
    "  features: int\n",
    "  reduction: int = 4\n",
    "  use_bias: bool = True\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    # 2D global average pooling\n",
    "    y = jnp.mean(x, axis=[1, 2], keepdims=True)\n",
    "    # Squeeze (in Squeeze-Excitation)\n",
    "    y = Conv1x1(self.features // self.reduction, use_bias=self.use_bias)(y)\n",
    "    y = nn.relu(y)\n",
    "    # Excitation (in Squeeze-Excitation)\n",
    "    y = Conv1x1(self.features, use_bias=self.use_bias)(y)\n",
    "    y = nn.sigmoid(y)\n",
    "    return x * y\n",
    "\n",
    "\n",
    "class RCAB(nn.Module):\n",
    "  \"\"\"Residual channel attention block. Contains LN,Conv,lRelu,Conv,SELayer.\"\"\"\n",
    "  features: int\n",
    "  reduction: int = 4\n",
    "  lrelu_slope: float = 0.2\n",
    "  use_bias: bool = True\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    shortcut = x\n",
    "    x = nn.LayerNorm(name=\"LayerNorm\")(x)\n",
    "    x = Conv3x3(features=self.features, use_bias=self.use_bias, name=\"conv1\")(x)\n",
    "    x = nn.leaky_relu(x, negative_slope=self.lrelu_slope)\n",
    "    x = Conv3x3(features=self.features, use_bias=self.use_bias, name=\"conv2\")(x)\n",
    "    x = CALayer(features=self.features, reduction=self.reduction,\n",
    "                use_bias=self.use_bias, name=\"channel_attention\")(x)\n",
    "    return x + shortcut\n",
    "\n",
    "\n",
    "class GridGatingUnit(nn.Module):\n",
    "  \"\"\"A SpatialGatingUnit as defined in the gMLP paper.\n",
    "\n",
    "  The 'spatial' dim is defined as the second last.\n",
    "  If applied on other dims, you should swapaxes first.\n",
    "  \"\"\"\n",
    "  use_bias: bool = True\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    u, v = jnp.split(x, 2, axis=-1)\n",
    "    v = nn.LayerNorm(name=\"intermediate_layernorm\")(v)\n",
    "    n = x.shape[-3]   # get spatial dim\n",
    "    v = jnp.swapaxes(v, -1, -3)\n",
    "    v = nn.Dense(n, use_bias=self.use_bias, kernel_init=weight_initializer)(v)\n",
    "    v = jnp.swapaxes(v, -1, -3)\n",
    "    return u * (v + 1.)\n",
    "\n",
    "\n",
    "class GridGmlpLayer(nn.Module):\n",
    "  \"\"\"Grid gMLP layer that performs global mixing of tokens.\"\"\"\n",
    "  grid_size: Sequence[int]\n",
    "  use_bias: bool = True\n",
    "  factor: int = 2\n",
    "  dropout_rate: float = 0.0\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x, deterministic=True):\n",
    "    n, h, w, num_channels = x.shape\n",
    "    gh, gw = self.grid_size\n",
    "    fh, fw = h // gh, w // gw\n",
    "    x = block_images_einops(x, patch_size=(fh, fw))\n",
    "    # gMLP1: Global (grid) mixing part, provides global grid communication.\n",
    "    y = nn.LayerNorm(name=\"LayerNorm\")(x)\n",
    "    y = nn.Dense(num_channels * self.factor, use_bias=self.use_bias,\n",
    "                 kernel_init=weight_initializer, name=\"in_project\")(y)\n",
    "    y = nn.gelu(y)\n",
    "    y = GridGatingUnit(use_bias=self.use_bias, name=\"GridGatingUnit\")(y)\n",
    "    y = nn.Dense(num_channels, use_bias=self.use_bias,\n",
    "                 kernel_init=weight_initializer, name=\"out_project\")(y)\n",
    "    y = nn.Dropout(self.dropout_rate)(y, deterministic)\n",
    "    x = x + y\n",
    "    x = unblock_images_einops(x, grid_size=(gh, gw), patch_size=(fh, fw))\n",
    "    return x\n",
    "\n",
    "\n",
    "class BlockGatingUnit(nn.Module):\n",
    "  \"\"\"A SpatialGatingUnit as defined in the gMLP paper.\n",
    "\n",
    "  The 'spatial' dim is defined as the **second last**.\n",
    "  If applied on other dims, you should swapaxes first.\n",
    "  \"\"\"\n",
    "  use_bias: bool = True\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    u, v = jnp.split(x, 2, axis=-1)\n",
    "    v = nn.LayerNorm(name=\"intermediate_layernorm\")(v)\n",
    "    n = x.shape[-2]  # get spatial dim\n",
    "    v = jnp.swapaxes(v, -1, -2)\n",
    "    v = nn.Dense(n, use_bias=self.use_bias, kernel_init=weight_initializer)(v)\n",
    "    v = jnp.swapaxes(v, -1, -2)\n",
    "    return u * (v + 1.)\n",
    "\n",
    "\n",
    "class BlockGmlpLayer(nn.Module):\n",
    "  \"\"\"Block gMLP layer that performs local mixing of tokens.\"\"\"\n",
    "  block_size: Sequence[int]\n",
    "  use_bias: bool = True\n",
    "  factor: int = 2\n",
    "  dropout_rate: float = 0.0\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x, deterministic=True):\n",
    "    n, h, w, num_channels = x.shape\n",
    "    fh, fw = self.block_size\n",
    "    gh, gw = h // fh, w // fw\n",
    "    x = block_images_einops(x, patch_size=(fh, fw))\n",
    "    # MLP2: Local (block) mixing part, provides within-block communication.\n",
    "    y = nn.LayerNorm(name=\"LayerNorm\")(x)\n",
    "    y = nn.Dense(num_channels * self.factor, use_bias=self.use_bias,\n",
    "                 kernel_init=weight_initializer, name=\"in_project\")(y)\n",
    "    y = nn.gelu(y)\n",
    "    y = BlockGatingUnit(use_bias=self.use_bias, name=\"BlockGatingUnit\")(y)\n",
    "    y = nn.Dense(num_channels, use_bias=self.use_bias,\n",
    "                 kernel_init=weight_initializer, name=\"out_project\")(y)\n",
    "    y = nn.Dropout(self.dropout_rate)(y, deterministic)\n",
    "    x = x + y\n",
    "    x = unblock_images_einops(x, grid_size=(gh, gw), patch_size=(fh, fw))\n",
    "    return x\n",
    "\n",
    "\n",
    "class ResidualSplitHeadMultiAxisGmlpLayer(nn.Module):\n",
    "  \"\"\"The multi-axis gated MLP block.\"\"\"\n",
    "  block_size: Sequence[int]\n",
    "  grid_size: Sequence[int]\n",
    "  block_gmlp_factor: int = 2\n",
    "  grid_gmlp_factor: int = 2\n",
    "  input_proj_factor: int = 2\n",
    "  use_bias: bool = True\n",
    "  dropout_rate: float = 0.0\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x, deterministic=True):\n",
    "    shortcut = x\n",
    "    n, h, w, num_channels = x.shape\n",
    "    x = nn.LayerNorm(name=\"LayerNorm_in\")(x)\n",
    "    x = nn.Dense(num_channels * self.input_proj_factor, use_bias=self.use_bias,\n",
    "                 kernel_init=weight_initializer, name=\"in_project\")(x)\n",
    "    x = nn.gelu(x)\n",
    "\n",
    "    u, v = jnp.split(x, 2, axis=-1)\n",
    "    # GridGMLPLayer\n",
    "    u = GridGmlpLayer(\n",
    "        grid_size=self.grid_size,\n",
    "        factor=self.grid_gmlp_factor,\n",
    "        use_bias=self.use_bias,\n",
    "        dropout_rate=self.dropout_rate,\n",
    "        name=\"GridGmlpLayer\")(u, deterministic)\n",
    "\n",
    "    # BlockGMLPLayer\n",
    "    v = BlockGmlpLayer(\n",
    "        block_size=self.block_size,\n",
    "        factor=self.block_gmlp_factor,\n",
    "        use_bias=self.use_bias,\n",
    "        dropout_rate=self.dropout_rate,\n",
    "        name=\"BlockGmlpLayer\")(v, deterministic)\n",
    "\n",
    "    x = jnp.concatenate([u, v], axis=-1)\n",
    "\n",
    "    x = nn.Dense(num_channels, use_bias=self.use_bias,\n",
    "                 kernel_init=weight_initializer, name=\"out_project\")(x)\n",
    "    x = nn.Dropout(self.dropout_rate)(x, deterministic)\n",
    "    x = x + shortcut\n",
    "    return x\n",
    "\n",
    "\n",
    "class RDCAB(nn.Module):\n",
    "  \"\"\"Residual dense channel attention block. Used in Bottlenecks.\"\"\"\n",
    "  features: int\n",
    "  reduction: int = 16\n",
    "  use_bias: bool = True\n",
    "  dropout_rate: float = 0.0\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x, deterministic=True):\n",
    "    y = nn.LayerNorm(name=\"LayerNorm\")(x)\n",
    "    y = MlpBlock(\n",
    "        mlp_dim=self.features,\n",
    "        dropout_rate=self.dropout_rate,\n",
    "        use_bias=self.use_bias,\n",
    "        name=\"channel_mixing\")(\n",
    "            y, deterministic=deterministic)\n",
    "    y = CALayer(\n",
    "        features=self.features,\n",
    "        reduction=self.reduction,\n",
    "        use_bias=self.use_bias,\n",
    "        name=\"channel_attention\")(\n",
    "            y)\n",
    "    x = x + y\n",
    "    return x\n",
    "\n",
    "\n",
    "class BottleneckBlock(nn.Module):\n",
    "  \"\"\"The bottleneck block consisting of multi-axis gMLP block and RDCAB.\"\"\"\n",
    "  features: int\n",
    "  block_size: Sequence[int]\n",
    "  grid_size: Sequence[int]\n",
    "  num_groups: int = 1\n",
    "  block_gmlp_factor: int = 2\n",
    "  grid_gmlp_factor: int = 2\n",
    "  input_proj_factor: int = 2\n",
    "  channels_reduction: int = 4\n",
    "  dropout_rate: float = 0.0\n",
    "  use_bias: bool = True\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x, deterministic):\n",
    "    \"\"\"Applies the Mixer block to inputs.\"\"\"\n",
    "    assert x.ndim == 4  # Input has shape [batch, h, w, c]\n",
    "    n, h, w, num_channels = x.shape\n",
    "\n",
    "    # input projection\n",
    "    x = Conv1x1(self.features, use_bias=self.use_bias, name=\"input_proj\")(x)\n",
    "    shortcut_long = x\n",
    "\n",
    "    for i in range(self.num_groups):\n",
    "      x = ResidualSplitHeadMultiAxisGmlpLayer(\n",
    "          grid_size=self.grid_size,\n",
    "          block_size=self.block_size,\n",
    "          grid_gmlp_factor=self.grid_gmlp_factor,\n",
    "          block_gmlp_factor=self.block_gmlp_factor,\n",
    "          input_proj_factor=self.input_proj_factor,\n",
    "          use_bias=self.use_bias,\n",
    "          dropout_rate=self.dropout_rate,\n",
    "          name=f\"SplitHeadMultiAxisGmlpLayer_{i}\")(x, deterministic)\n",
    "      # Channel-mixing part, which provides within-patch communication.\n",
    "      x = RDCAB(\n",
    "          features=self.features,\n",
    "          reduction=self.channels_reduction,\n",
    "          use_bias=self.use_bias,\n",
    "          name=f\"channel_attention_block_1_{i}\")(\n",
    "              x)\n",
    "\n",
    "    # long skip-connect\n",
    "    x = x + shortcut_long\n",
    "    return x\n",
    "\n",
    "\n",
    "class UNetEncoderBlock(nn.Module):\n",
    "  \"\"\"Encoder block in MAXIM.\"\"\"\n",
    "  features: int\n",
    "  block_size: Sequence[int]\n",
    "  grid_size: Sequence[int]\n",
    "  num_groups: int = 1\n",
    "  lrelu_slope: float = 0.2\n",
    "  block_gmlp_factor: int = 2\n",
    "  grid_gmlp_factor: int = 2\n",
    "  input_proj_factor: int = 2\n",
    "  channels_reduction: int = 4\n",
    "  dropout_rate: float = 0.0\n",
    "  downsample: bool = True\n",
    "  use_global_mlp: bool = True\n",
    "  use_bias: bool = True\n",
    "  use_cross_gating: bool = False\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x: jnp.ndarray, skip: jnp.ndarray = None,\n",
    "               enc: jnp.ndarray = None, dec: jnp.ndarray = None, *,\n",
    "               deterministic: bool = True) -> jnp.ndarray:\n",
    "    if skip is not None:\n",
    "      x = jnp.concatenate([x, skip], axis=-1)\n",
    "\n",
    "    # convolution-in\n",
    "    x = Conv1x1(self.features, use_bias=self.use_bias)(x)\n",
    "    shortcut_long = x\n",
    "\n",
    "    for i in range(self.num_groups):\n",
    "      if self.use_global_mlp:\n",
    "        x = ResidualSplitHeadMultiAxisGmlpLayer(\n",
    "            grid_size=self.grid_size,\n",
    "            block_size=self.block_size,\n",
    "            grid_gmlp_factor=self.grid_gmlp_factor,\n",
    "            block_gmlp_factor=self.block_gmlp_factor,\n",
    "            input_proj_factor=self.input_proj_factor,\n",
    "            use_bias=self.use_bias,\n",
    "            dropout_rate=self.dropout_rate,\n",
    "            name=f\"SplitHeadMultiAxisGmlpLayer_{i}\")(x, deterministic)\n",
    "      x = RCAB(\n",
    "          features=self.features,\n",
    "          reduction=self.channels_reduction,\n",
    "          use_bias=self.use_bias,\n",
    "          name=f\"channel_attention_block_1{i}\")(x)\n",
    "\n",
    "    x = x + shortcut_long\n",
    "\n",
    "    if enc is not None and dec is not None:\n",
    "      assert self.use_cross_gating\n",
    "      x, _ = CrossGatingBlock(\n",
    "          features=self.features,\n",
    "          block_size=self.block_size,\n",
    "          grid_size=self.grid_size,\n",
    "          dropout_rate=self.dropout_rate,\n",
    "          input_proj_factor=self.input_proj_factor,\n",
    "          upsample_y=False,\n",
    "          use_bias=self.use_bias,\n",
    "          name=\"cross_gating_block\")(\n",
    "              x, enc + dec, deterministic=deterministic)\n",
    "\n",
    "    if self.downsample:\n",
    "      x_down = Conv_down(self.features, use_bias=self.use_bias)(x)\n",
    "      return x_down, x\n",
    "    else:\n",
    "      return x\n",
    "\n",
    "\n",
    "class UNetDecoderBlock(nn.Module):\n",
    "  \"\"\"Decoder block in MAXIM.\"\"\"\n",
    "  features: int\n",
    "  block_size: Sequence[int]\n",
    "  grid_size: Sequence[int]\n",
    "  num_groups: int = 1\n",
    "  lrelu_slope: float = 0.2\n",
    "  block_gmlp_factor: int = 2\n",
    "  grid_gmlp_factor: int = 2\n",
    "  input_proj_factor: int = 2\n",
    "  channels_reduction: int = 4\n",
    "  dropout_rate: float = 0.0\n",
    "  downsample: bool = True\n",
    "  use_global_mlp: bool = True\n",
    "  use_bias: bool = True\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x: jnp.ndarray, bridge: jnp.ndarray = None,\n",
    "               deterministic: bool = True) -> jnp.ndarray:\n",
    "    x = ConvT_up(self.features, use_bias=self.use_bias)(x)\n",
    "\n",
    "    x = UNetEncoderBlock(\n",
    "        self.features,\n",
    "        num_groups=self.num_groups,\n",
    "        lrelu_slope=self.lrelu_slope,\n",
    "        block_size=self.block_size,\n",
    "        grid_size=self.grid_size,\n",
    "        block_gmlp_factor=self.block_gmlp_factor,\n",
    "        grid_gmlp_factor=self.grid_gmlp_factor,\n",
    "        channels_reduction=self.channels_reduction,\n",
    "        use_global_mlp=self.use_global_mlp,\n",
    "        dropout_rate=self.dropout_rate,\n",
    "        downsample=False,\n",
    "        use_bias=self.use_bias)(x, skip=bridge, deterministic=deterministic)\n",
    "    return x\n",
    "\n",
    "\n",
    "class GetSpatialGatingWeights(nn.Module):\n",
    "  \"\"\"Get gating weights for cross-gating MLP block.\"\"\"\n",
    "  features: int\n",
    "  block_size: Sequence[int]\n",
    "  grid_size: Sequence[int]\n",
    "  input_proj_factor: int = 2\n",
    "  dropout_rate: float = 0.0\n",
    "  use_bias: bool = True\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x, deterministic):\n",
    "    n, h, w, num_channels = x.shape\n",
    "\n",
    "    # input projection\n",
    "    x = nn.LayerNorm(name=\"LayerNorm_in\")(x)\n",
    "    x = nn.Dense(\n",
    "        num_channels * self.input_proj_factor,\n",
    "        use_bias=self.use_bias,\n",
    "        name=\"in_project\")(\n",
    "            x)\n",
    "    x = nn.gelu(x)\n",
    "    u, v = jnp.split(x, 2, axis=-1)\n",
    "\n",
    "    # Get grid MLP weights\n",
    "    gh, gw = self.grid_size\n",
    "    fh, fw = h // gh, w // gw\n",
    "    u = block_images_einops(u, patch_size=(fh, fw))\n",
    "    dim_u = u.shape[-3]\n",
    "    u = jnp.swapaxes(u, -1, -3)\n",
    "    u = nn.Dense(\n",
    "        dim_u, use_bias=self.use_bias, kernel_init=nn.initializers.normal(2e-2),\n",
    "        bias_init=nn.initializers.ones)(u)\n",
    "    u = jnp.swapaxes(u, -1, -3)\n",
    "    u = unblock_images_einops(u, grid_size=(gh, gw), patch_size=(fh, fw))\n",
    "\n",
    "    # Get Block MLP weights\n",
    "    fh, fw = self.block_size\n",
    "    gh, gw = h // fh, w // fw\n",
    "    v = block_images_einops(v, patch_size=(fh, fw))\n",
    "    dim_v = v.shape[-2]\n",
    "    v = jnp.swapaxes(v, -1, -2)\n",
    "    v = nn.Dense(\n",
    "        dim_v, use_bias=self.use_bias, kernel_init=nn.initializers.normal(2e-2),\n",
    "        bias_init=nn.initializers.ones)(v)\n",
    "    v = jnp.swapaxes(v, -1, -2)\n",
    "    v = unblock_images_einops(v, grid_size=(gh, gw), patch_size=(fh, fw))\n",
    "\n",
    "    x = jnp.concatenate([u, v], axis=-1)\n",
    "    x = nn.Dense(num_channels, use_bias=self.use_bias, name=\"out_project\")(x)\n",
    "    x = nn.Dropout(self.dropout_rate)(x, deterministic)\n",
    "    return x\n",
    "\n",
    "\n",
    "class CrossGatingBlock(nn.Module):\n",
    "  \"\"\"Cross-gating MLP block.\"\"\"\n",
    "  features: int\n",
    "  block_size: Sequence[int]\n",
    "  grid_size: Sequence[int]\n",
    "  dropout_rate: float = 0.0\n",
    "  input_proj_factor: int = 2\n",
    "  upsample_y: bool = True\n",
    "  use_bias: bool = True\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x, y, deterministic=True):\n",
    "    # Upscale Y signal, y is the gating signal.\n",
    "    if self.upsample_y:\n",
    "      y = ConvT_up(self.features, use_bias=self.use_bias)(y)\n",
    "\n",
    "    x = Conv1x1(self.features, use_bias=self.use_bias)(x)\n",
    "    n, h, w, num_channels = x.shape\n",
    "    y = Conv1x1(num_channels, use_bias=self.use_bias)(y)\n",
    "\n",
    "    assert y.shape == x.shape\n",
    "    shortcut_x = x\n",
    "    shortcut_y = y\n",
    "\n",
    "    # Get gating weights from X\n",
    "    x = nn.LayerNorm(name=\"LayerNorm_x\")(x)\n",
    "    x = nn.Dense(num_channels, use_bias=self.use_bias, name=\"in_project_x\")(x)\n",
    "    x = nn.gelu(x)\n",
    "    gx = GetSpatialGatingWeights(\n",
    "        features=num_channels,\n",
    "        block_size=self.block_size,\n",
    "        grid_size=self.grid_size,\n",
    "        dropout_rate=self.dropout_rate,\n",
    "        use_bias=self.use_bias,\n",
    "        name=\"SplitHeadMultiAxisGating_x\")(\n",
    "            x, deterministic=deterministic)\n",
    "\n",
    "    # Get gating weights from Y\n",
    "    y = nn.LayerNorm(name=\"LayerNorm_y\")(y)\n",
    "    y = nn.Dense(num_channels, use_bias=self.use_bias, name=\"in_project_y\")(y)\n",
    "    y = nn.gelu(y)\n",
    "    gy = GetSpatialGatingWeights(\n",
    "        features=num_channels,\n",
    "        block_size=self.block_size,\n",
    "        grid_size=self.grid_size,\n",
    "        dropout_rate=self.dropout_rate,\n",
    "        use_bias=self.use_bias,\n",
    "        name=\"SplitHeadMultiAxisGating_y\")(\n",
    "            y, deterministic=deterministic)\n",
    "\n",
    "    # Apply cross gating: X = X * GY, Y = Y * GX\n",
    "    y = y * gx\n",
    "    y = nn.Dense(num_channels, use_bias=self.use_bias, name=\"out_project_y\")(y)\n",
    "    y = nn.Dropout(self.dropout_rate)(y, deterministic=deterministic)\n",
    "    y = y + shortcut_y\n",
    "\n",
    "    x = x * gy  # gating x using y\n",
    "    x = nn.Dense(num_channels, use_bias=self.use_bias, name=\"out_project_x\")(x)\n",
    "    x = nn.Dropout(self.dropout_rate)(x, deterministic=deterministic)\n",
    "    x = x + y + shortcut_x  # get all aggregated signals\n",
    "    return x, y\n",
    "\n",
    "\n",
    "class SAM(nn.Module):\n",
    "  \"\"\"Supervised attention module for multi-stage training.\n",
    "\n",
    "  Introduced by MPRNet [CVPR2021]: https://github.com/swz30/MPRNet\n",
    "  \"\"\"\n",
    "  features: int\n",
    "  output_channels: int = 3\n",
    "  use_bias: bool = True\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x: jnp.ndarray, x_image: jnp.ndarray, *,\n",
    "               train: bool) -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
    "    \"\"\"Apply the SAM module to the input and features.\n",
    "\n",
    "    Args:\n",
    "      x: the output features from UNet decoder with shape (h, w, c)\n",
    "      x_image: the input image with shape (h, w, 3)\n",
    "      train: Whether it is training\n",
    "\n",
    "    Returns:\n",
    "      A tuple of tensors (x1, image) where (x1) is the sam features used for the\n",
    "        next stage, and (image) is the output restored image at current stage.\n",
    "    \"\"\"\n",
    "    # Get features\n",
    "    x1 = Conv3x3(self.features, use_bias=self.use_bias)(x)\n",
    "\n",
    "    # Output restored image X_s\n",
    "    if self.output_channels == 3:\n",
    "      image = Conv3x3(self.output_channels, use_bias=self.use_bias)(x) + x_image\n",
    "    else:\n",
    "      image = Conv3x3(self.output_channels, use_bias=self.use_bias)(x)\n",
    "\n",
    "    # Get attention maps for features\n",
    "    x2 = nn.sigmoid(Conv3x3(self.features, use_bias=self.use_bias)(image))\n",
    "\n",
    "    # Get attended feature maps\n",
    "    x1 = x1 * x2\n",
    "\n",
    "    # Residual connection\n",
    "    x1 = x1 + x\n",
    "    return x1, image\n",
    "\n",
    "\n",
    "class MAXIM(nn.Module):\n",
    "  \"\"\"The MAXIM model function with multi-stage and multi-scale supervision.\n",
    "\n",
    "  For more model details, please check the CVPR paper:\n",
    "  MAXIM: MUlti-Axis MLP for Image Processing (https://arxiv.org/abs/2201.02973)\n",
    "\n",
    "  Attributes:\n",
    "    features: initial hidden dimension for the input resolution.\n",
    "    depth: the number of downsampling depth for the model.\n",
    "    num_stages: how many stages to use. It will also affects the output list.\n",
    "    num_groups: how many blocks each stage contains.\n",
    "    use_bias: whether to use bias in all the conv/mlp layers.\n",
    "    num_supervision_scales: the number of desired supervision scales.\n",
    "    lrelu_slope: the negative slope parameter in leaky_relu layers.\n",
    "    use_global_mlp: whether to use the multi-axis gated MLP block (MAB) in each\n",
    "      layer.\n",
    "    use_cross_gating: whether to use the cross-gating MLP block (CGB) in the\n",
    "      skip connections and multi-stage feature fusion layers.\n",
    "    high_res_stages: how many stages are specificied as high-res stages. The\n",
    "      rest (depth - high_res_stages) are called low_res_stages.\n",
    "    block_size_hr: the block_size parameter for high-res stages.\n",
    "    block_size_lr: the block_size parameter for low-res stages.\n",
    "    grid_size_hr: the grid_size parameter for high-res stages.\n",
    "    grid_size_lr: the grid_size parameter for low-res stages.\n",
    "    num_bottleneck_blocks: how many bottleneck blocks.\n",
    "    block_gmlp_factor: the input projection factor for block_gMLP layers.\n",
    "    grid_gmlp_factor: the input projection factor for grid_gMLP layers.\n",
    "    input_proj_factor: the input projection factor for the MAB block.\n",
    "    channels_reduction: the channel reduction factor for SE layer.\n",
    "    num_outputs: the output channels.\n",
    "    dropout_rate: Dropout rate.\n",
    "\n",
    "  Returns:\n",
    "    The output contains a list of arrays consisting of multi-stage multi-scale\n",
    "    outputs. For example, if num_stages = num_supervision_scales = 3 (the\n",
    "    model used in the paper), the output specs are: outputs =\n",
    "    [[output_stage1_scale1, output_stage1_scale2, output_stage1_scale3],\n",
    "     [output_stage2_scale1, output_stage2_scale2, output_stage2_scale3],\n",
    "     [output_stage3_scale1, output_stage3_scale2, output_stage3_scale3],]\n",
    "    The final output can be retrieved by outputs[-1][-1].\n",
    "  \"\"\"\n",
    "  features: int = 64\n",
    "  depth: int = 3\n",
    "  num_stages: int = 2\n",
    "  num_groups: int = 1\n",
    "  use_bias: bool = True\n",
    "  num_supervision_scales: int = 1\n",
    "  lrelu_slope: float = 0.2\n",
    "  use_global_mlp: bool = True\n",
    "  use_cross_gating: bool = True\n",
    "  high_res_stages: int = 2\n",
    "  block_size_hr: Sequence[int] = (16, 16)\n",
    "  block_size_lr: Sequence[int] = (8, 8)\n",
    "  grid_size_hr: Sequence[int] = (16, 16)\n",
    "  grid_size_lr: Sequence[int] = (8, 8)\n",
    "  num_bottleneck_blocks: int = 1\n",
    "  block_gmlp_factor: int = 2\n",
    "  grid_gmlp_factor: int = 2\n",
    "  input_proj_factor: int = 2\n",
    "  channels_reduction: int = 4\n",
    "  num_outputs: int = 3\n",
    "  dropout_rate: float = 0.0\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x: jnp.ndarray, *, train: bool = False) -> Any:\n",
    "\n",
    "    n, h, w, c = x.shape  # input image shape\n",
    "    shortcuts = []\n",
    "    shortcuts.append(x)\n",
    "    # Get multi-scale input images\n",
    "    for i in range(1, self.num_supervision_scales):\n",
    "      shortcuts.append(jax.image.resize(\n",
    "          x, shape=(n, h // (2**i), w // (2**i), c), method=\"nearest\"))\n",
    "\n",
    "    # store outputs from all stages and all scales\n",
    "    # Eg, [[(64, 64, 3), (128, 128, 3), (256, 256, 3)],   # Stage-1 outputs\n",
    "    #      [(64, 64, 3), (128, 128, 3), (256, 256, 3)],]  # Stage-2 outputs\n",
    "    outputs_all = []\n",
    "    sam_features, encs_prev, decs_prev = [], [], []\n",
    "\n",
    "    for idx_stage in range(self.num_stages):\n",
    "      # Input convolution, get multi-scale input features\n",
    "      x_scales = []\n",
    "      for i in range(self.num_supervision_scales):\n",
    "        x_scale = Conv3x3(\n",
    "            (2**i) * self.features,\n",
    "            use_bias=self.use_bias,\n",
    "            name=f\"stage_{idx_stage}_input_conv_{i}\")(\n",
    "                shortcuts[i])\n",
    "\n",
    "        # If later stages, fuse input features with SAM features from prev stage\n",
    "        if idx_stage > 0:\n",
    "          # use larger blocksize at high-res stages\n",
    "          if self.use_cross_gating:\n",
    "            block_size = self.block_size_hr if i < self.high_res_stages else self.block_size_lr\n",
    "            grid_size = self.grid_size_hr if i < self.high_res_stages else self.block_size_lr\n",
    "            x_scale, _ = CrossGatingBlock(\n",
    "                features=(2**i) * self.features,\n",
    "                block_size=block_size,\n",
    "                grid_size=grid_size,\n",
    "                dropout_rate=self.dropout_rate,\n",
    "                input_proj_factor=self.input_proj_factor,\n",
    "                upsample_y=False,\n",
    "                use_bias=self.use_bias,\n",
    "                name=f\"stage_{idx_stage}_input_fuse_sam_{i}\")(\n",
    "                    x_scale, sam_features.pop(), deterministic=not train)\n",
    "          else:\n",
    "            x_scale = Conv1x1(\n",
    "                (2**i) * self.features,\n",
    "                use_bias=self.use_bias,\n",
    "                name=f\"stage_{idx_stage}_input_catconv_{i}\")(\n",
    "                    jnp.concatenate(\n",
    "                        [x_scale, sam_features.pop()], axis=-1))\n",
    "\n",
    "        x_scales.append(x_scale)\n",
    "\n",
    "      # start encoder blocks\n",
    "      encs = []\n",
    "      x = x_scales[0]  # First full-scale input feature\n",
    "\n",
    "      for i in range(self.depth):  # 0, 1, 2\n",
    "        # use larger blocksize at high-res stages, vice versa.\n",
    "        block_size = self.block_size_hr if i < self.high_res_stages else self.block_size_lr\n",
    "        grid_size = self.grid_size_hr if i < self.high_res_stages else self.block_size_lr\n",
    "        use_cross_gating_layer = True if idx_stage > 0 else False\n",
    "\n",
    "        # Multi-scale input if multi-scale supervision\n",
    "        x_scale = x_scales[i] if i < self.num_supervision_scales else None\n",
    "\n",
    "        # UNet Encoder block\n",
    "        enc_prev = encs_prev.pop() if idx_stage > 0 else None\n",
    "        dec_prev = decs_prev.pop() if idx_stage > 0 else None\n",
    "\n",
    "        x, bridge = UNetEncoderBlock(\n",
    "            features=(2**i) * self.features,\n",
    "            num_groups=self.num_groups,\n",
    "            downsample=True,\n",
    "            lrelu_slope=self.lrelu_slope,\n",
    "            block_size=block_size,\n",
    "            grid_size=grid_size,\n",
    "            block_gmlp_factor=self.block_gmlp_factor,\n",
    "            grid_gmlp_factor=self.grid_gmlp_factor,\n",
    "            input_proj_factor=self.input_proj_factor,\n",
    "            channels_reduction=self.channels_reduction,\n",
    "            use_global_mlp=self.use_global_mlp,\n",
    "            dropout_rate=self.dropout_rate,\n",
    "            use_bias=self.use_bias,\n",
    "            use_cross_gating=use_cross_gating_layer,\n",
    "            name=f\"stage_{idx_stage}_encoder_block_{i}\")(\n",
    "                x,\n",
    "                skip=x_scale,\n",
    "                enc=enc_prev,\n",
    "                dec=dec_prev,\n",
    "                deterministic=not train)\n",
    "\n",
    "        # Cache skip signals\n",
    "        encs.append(bridge)\n",
    "\n",
    "      # Global MLP bottleneck blocks\n",
    "      for i in range(self.num_bottleneck_blocks):\n",
    "        x = BottleneckBlock(\n",
    "            block_size=self.block_size_lr,\n",
    "            grid_size=self.block_size_lr,\n",
    "            features=(2**(self.depth - 1)) * self.features,\n",
    "            num_groups=self.num_groups,\n",
    "            block_gmlp_factor=self.block_gmlp_factor,\n",
    "            grid_gmlp_factor=self.grid_gmlp_factor,\n",
    "            input_proj_factor=self.input_proj_factor,\n",
    "            dropout_rate=self.dropout_rate,\n",
    "            use_bias=self.use_bias,\n",
    "            channels_reduction=self.channels_reduction,\n",
    "            name=f\"stage_{idx_stage}_global_block_{i}\")(\n",
    "                x, deterministic=not train)\n",
    "      # cache global feature for cross-gating\n",
    "      global_feature = x\n",
    "\n",
    "      # start cross gating. Use multi-scale feature fusion\n",
    "      skip_features = []\n",
    "      for i in reversed(range(self.depth)):  # 2, 1, 0\n",
    "        # use larger blocksize at high-res stages\n",
    "        block_size = self.block_size_hr if i < self.high_res_stages else self.block_size_lr\n",
    "        grid_size = self.grid_size_hr if i < self.high_res_stages else self.block_size_lr\n",
    "\n",
    "        # get additional multi-scale signals\n",
    "        signal = jnp.concatenate([\n",
    "            UpSampleRatio(\n",
    "                (2**i) * self.features,\n",
    "                ratio=2**(j - i),\n",
    "                use_bias=self.use_bias)(enc) for j, enc in enumerate(encs)\n",
    "        ],\n",
    "                                 axis=-1)\n",
    "\n",
    "        # Use cross-gating to cross modulate features\n",
    "        if self.use_cross_gating:\n",
    "          skips, global_feature = CrossGatingBlock(\n",
    "              features=(2**i) * self.features,\n",
    "              block_size=block_size,\n",
    "              grid_size=grid_size,\n",
    "              input_proj_factor=self.input_proj_factor,\n",
    "              dropout_rate=self.dropout_rate,\n",
    "              upsample_y=True,\n",
    "              use_bias=self.use_bias,\n",
    "              name=f\"stage_{idx_stage}_cross_gating_block_{i}\")(\n",
    "                  signal, global_feature, deterministic=not train)\n",
    "        else:\n",
    "          skips = Conv1x1(\n",
    "              (2**i) * self.features, use_bias=self.use_bias)(\n",
    "                  signal)\n",
    "          skips = Conv3x3((2**i) * self.features, use_bias=self.use_bias)(skips)\n",
    "\n",
    "        skip_features.append(skips)\n",
    "\n",
    "      # start decoder. Multi-scale feature fusion of cross-gated features\n",
    "      outputs, decs, sam_features = [], [], []\n",
    "      for i in reversed(range(self.depth)):\n",
    "        # use larger blocksize at high-res stages\n",
    "        block_size = self.block_size_hr if i < self.high_res_stages else self.block_size_lr\n",
    "        grid_size = self.grid_size_hr if i < self.high_res_stages else self.block_size_lr\n",
    "\n",
    "        # get multi-scale skip signals from cross-gating block\n",
    "        signal = jnp.concatenate([\n",
    "            UpSampleRatio(\n",
    "                (2**i) * self.features,\n",
    "                ratio=2**(self.depth - j - 1 - i),\n",
    "                use_bias=self.use_bias)(skip)\n",
    "            for j, skip in enumerate(skip_features)\n",
    "        ],\n",
    "                                 axis=-1)\n",
    "\n",
    "        # Decoder block\n",
    "        x = UNetDecoderBlock(\n",
    "            features=(2**i) * self.features,\n",
    "            num_groups=self.num_groups,\n",
    "            lrelu_slope=self.lrelu_slope,\n",
    "            block_size=block_size,\n",
    "            grid_size=grid_size,\n",
    "            block_gmlp_factor=self.block_gmlp_factor,\n",
    "            grid_gmlp_factor=self.grid_gmlp_factor,\n",
    "            input_proj_factor=self.input_proj_factor,\n",
    "            channels_reduction=self.channels_reduction,\n",
    "            use_global_mlp=self.use_global_mlp,\n",
    "            dropout_rate=self.dropout_rate,\n",
    "            use_bias=self.use_bias,\n",
    "            name=f\"stage_{idx_stage}_decoder_block_{i}\")(\n",
    "                x, bridge=signal, deterministic=not train)\n",
    "\n",
    "        # Cache decoder features for later-stage's usage\n",
    "        decs.append(x)\n",
    "\n",
    "        # output conv, if not final stage, use supervised-attention-block.\n",
    "        if i < self.num_supervision_scales:\n",
    "          if idx_stage < self.num_stages - 1:  # not last stage, apply SAM\n",
    "            sam, output = SAM(\n",
    "                (2**i) * self.features,\n",
    "                output_channels=self.num_outputs,\n",
    "                use_bias=self.use_bias,\n",
    "                name=f\"stage_{idx_stage}_supervised_attention_module_{i}\")(\n",
    "                    x, shortcuts[i], train=train)\n",
    "            outputs.append(output)\n",
    "            sam_features.append(sam)\n",
    "          else:  # Last stage, apply output convolutions\n",
    "            output = Conv3x3(self.num_outputs,\n",
    "                             use_bias=self.use_bias,\n",
    "                             name=f\"stage_{idx_stage}_output_conv_{i}\")(x)\n",
    "            output = output + shortcuts[i]\n",
    "            outputs.append(output)\n",
    "      # Cache encoder and decoder features for later-stage's usage\n",
    "      encs_prev = encs[::-1]\n",
    "      decs_prev = decs\n",
    "\n",
    "      # Store outputs\n",
    "      outputs_all.append(outputs)\n",
    "    return outputs_all\n",
    "\n",
    "\n",
    "def Model(*, variant=None, **kw):\n",
    "  \"\"\"Factory function to easily create a Model variant like \"S\".\n",
    "\n",
    "  Every model file should have this Model() function that returns the flax\n",
    "  model function. The function name should be fixed.\n",
    "\n",
    "  Args:\n",
    "    variant: UNet model variants. Options: 'S-1' | 'S-2' | 'S-3'\n",
    "        | 'M-1' | 'M-2' | 'M-3'\n",
    "    **kw: Other UNet config dicts.\n",
    "\n",
    "  Returns:\n",
    "    The MAXIM() model function\n",
    "  \"\"\"\n",
    "\n",
    "  if variant is not None:\n",
    "    config = {\n",
    "        # params: 6.108515000000001 M, GFLOPS: 93.163716608\n",
    "        \"S-1\": {\n",
    "            \"features\": 32,\n",
    "            \"depth\": 3,\n",
    "            \"num_stages\": 1,\n",
    "            \"num_groups\": 2,\n",
    "            \"num_bottleneck_blocks\": 2,\n",
    "            \"block_gmlp_factor\": 2,\n",
    "            \"grid_gmlp_factor\": 2,\n",
    "            \"input_proj_factor\": 2,\n",
    "            \"channels_reduction\": 4,\n",
    "        },\n",
    "        # params: 13.35383 M, GFLOPS: 206.743273472\n",
    "        \"S-2\": {\n",
    "            \"features\": 32,\n",
    "            \"depth\": 3,\n",
    "            \"num_stages\": 2,\n",
    "            \"num_groups\": 2,\n",
    "            \"num_bottleneck_blocks\": 2,\n",
    "            \"block_gmlp_factor\": 2,\n",
    "            \"grid_gmlp_factor\": 2,\n",
    "            \"input_proj_factor\": 2,\n",
    "            \"channels_reduction\": 4,\n",
    "        },\n",
    "        # params: 20.599145 M, GFLOPS: 320.32194560000005\n",
    "        \"S-3\": {\n",
    "            \"features\": 32,\n",
    "            \"depth\": 3,\n",
    "            \"num_stages\": 3,\n",
    "            \"num_groups\": 2,\n",
    "            \"num_bottleneck_blocks\": 2,\n",
    "            \"block_gmlp_factor\": 2,\n",
    "            \"grid_gmlp_factor\": 2,\n",
    "            \"input_proj_factor\": 2,\n",
    "            \"channels_reduction\": 4,\n",
    "        },\n",
    "        # params: 19.361219000000002 M, 308.495712256 GFLOPs\n",
    "        \"M-1\": {\n",
    "            \"features\": 64,\n",
    "            \"depth\": 3,\n",
    "            \"num_stages\": 1,\n",
    "            \"num_groups\": 2,\n",
    "            \"num_bottleneck_blocks\": 2,\n",
    "            \"block_gmlp_factor\": 2,\n",
    "            \"grid_gmlp_factor\": 2,\n",
    "            \"input_proj_factor\": 2,\n",
    "            \"channels_reduction\": 4,\n",
    "        },\n",
    "        # params: 40.83911 M, 675.25541888 GFLOPs\n",
    "        \"M-2\": {\n",
    "            \"features\": 64,\n",
    "            \"depth\": 3,\n",
    "            \"num_stages\": 2,\n",
    "            \"num_groups\": 2,\n",
    "            \"num_bottleneck_blocks\": 2,\n",
    "            \"block_gmlp_factor\": 2,\n",
    "            \"grid_gmlp_factor\": 2,\n",
    "            \"input_proj_factor\": 2,\n",
    "            \"channels_reduction\": 4,\n",
    "        },\n",
    "        # params: 62.317001 M, 1042.014666752 GFLOPs\n",
    "        \"M-3\": {\n",
    "            \"features\": 64,\n",
    "            \"depth\": 3,\n",
    "            \"num_stages\": 3,\n",
    "            \"num_groups\": 2,\n",
    "            \"num_bottleneck_blocks\": 2,\n",
    "            \"block_gmlp_factor\": 2,\n",
    "            \"grid_gmlp_factor\": 2,\n",
    "            \"input_proj_factor\": 2,\n",
    "            \"channels_reduction\": 4,\n",
    "        },\n",
    "    }[variant]\n",
    "\n",
    "    for k, v in config.items():\n",
    "      kw.setdefault(k, v)\n",
    "\n",
    "  return MAXIM(**kw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e92b49",
   "metadata": {},
   "source": [
    "## Assertion of outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3af3ac18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from maxim.layers import Resizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad1057b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_inputs = tf.random.normal((1, 365, 385, 3))\n",
    "\n",
    "resizing_tf = Resizing(256, 256, method=\"bilinear\", antialias=True)\n",
    "\n",
    "tf_outputs = resizing_tf(dummy_inputs).numpy()\n",
    "# tf_outputs = tf.image.resize(dummy_inputs, (256, 256), method=\"bilinear\").numpy()\n",
    "jax_outputs = jax.image.resize(dummy_inputs.numpy(), (1, 256, 256, 3), method=\"bilinear\")\n",
    "\n",
    "np.testing.assert_allclose(tf_outputs, np.array(jax_outputs), rtol=1e-5, atol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b49b3802",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_initializer = nn.initializers.ones\n",
    "\n",
    "class MlpBlock(nn.Module):\n",
    "  \"\"\"A 1-hidden-layer MLP block, applied over the last dimension.\"\"\"\n",
    "  mlp_dim: int\n",
    "  dropout_rate: float = 0.0\n",
    "  use_bias: bool = True\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x, deterministic=True):\n",
    "    n, h, w, d = x.shape\n",
    "    x = nn.Dense(self.mlp_dim, use_bias=self.use_bias,\n",
    "                 kernel_init=weight_initializer, bias_init=weight_initializer)(x)\n",
    "    x = nn.gelu(x)\n",
    "    x = nn.Dropout(rate=self.dropout_rate)(x, deterministic)\n",
    "    x = nn.Dense(d, use_bias=self.use_bias,\n",
    "                 kernel_init=weight_initializer, bias_init=weight_initializer)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1164159",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K \n",
    "from tensorflow.keras import layers \n",
    "\n",
    "def MlpBlockTF(\n",
    "    mlp_dim: int,\n",
    "    dropout_rate: float = 0.0,\n",
    "    use_bias: bool = True,\n",
    "    name: str = \"mlp_block\",\n",
    "):\n",
    "    \"\"\"A 1-hidden-layer MLP block, applied over the last dimension.\"\"\"\n",
    "\n",
    "    def apply(x):\n",
    "        d = K.int_shape(x)[-1]\n",
    "        x = layers.Dense(mlp_dim, use_bias=use_bias, kernel_initializer=\"ones\", bias_initializer=\"ones\", name=f\"{name}_Dense_0\")(x)\n",
    "        x = tf.nn.gelu(x, approximate=True)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        x = layers.Dense(d, use_bias=use_bias, name=f\"{name}_Dense_1\", kernel_initializer=\"ones\", bias_initializer=\"ones\")(x)\n",
    "        return x\n",
    "\n",
    "    return apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1bd28e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_inputs = tf.random.normal((1, 256, 256, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67e2bcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import random\n",
    "\n",
    "mlp_block_jax = MlpBlock(128)\n",
    "variables = mlp_block_jax.init(random.PRNGKey(0), dummy_inputs.numpy())\n",
    "jax_outputs = mlp_block_jax.apply(variables, dummy_inputs.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20e25384",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_block_tf = MlpBlockTF(128)\n",
    "tf_outputs = mlp_block_tf(dummy_inputs.numpy())\n",
    "\n",
    "np.testing.assert_allclose(tf_outputs, np.array(jax_outputs), rtol=1e-4, atol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b7b6db4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpSampleRatio(nn.Module):\n",
    "  \"\"\"Upsample features given a ratio > 0.\"\"\"\n",
    "  features: int\n",
    "  ratio: float\n",
    "  use_bias: bool = True\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    n, h, w, c = x.shape\n",
    "    x = jax.image.resize(\n",
    "        x,\n",
    "        shape=(n, int(h * self.ratio), int(w * self.ratio), c),\n",
    "        method=\"bilinear\")\n",
    "    x = Conv1x1(features=self.features, use_bias=self.use_bias, kernel_init=weight_initializer, bias_init=weight_initializer)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3c4e7e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "Conv1x1TF = functools.partial(layers.Conv2D, kernel_size=(1, 1), padding=\"same\")\n",
    "\n",
    "\n",
    "def UpSampleRatioTF(\n",
    "    num_channels: int, ratio: float, use_bias: bool = True, name: str = \"upsample\"\n",
    "):\n",
    "    \"\"\"Upsample features given a ratio > 0.\"\"\"\n",
    "\n",
    "    def apply(x):\n",
    "        n, h, w, c = (\n",
    "            K.int_shape(x)[0],\n",
    "            K.int_shape(x)[1],\n",
    "            K.int_shape(x)[2],\n",
    "            K.int_shape(x)[3],\n",
    "        )\n",
    "\n",
    "        # Following `jax.image.resize()`\n",
    "        x = Resizing(\n",
    "            height=tf.cast(h * ratio, tf.int32),\n",
    "            width=tf.cast(w * ratio, tf.int32),\n",
    "            method=\"bilinear\",\n",
    "            antialias=True,\n",
    "        )(x)\n",
    "\n",
    "        x = Conv1x1TF(filters=num_channels, use_bias=use_bias, name=f\"{name}_Conv_0\", kernel_initializer=\"ones\", bias_initializer=\"ones\")(x)\n",
    "        return x\n",
    "\n",
    "    return apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "71f5e444",
   "metadata": {},
   "outputs": [],
   "source": [
    "upsample_block_jax = UpSampleRatio(features=128, ratio=2)\n",
    "variables = upsample_block_jax.init(random.PRNGKey(0), dummy_inputs.numpy())\n",
    "jax_outputs = upsample_block_jax.apply(variables, dummy_inputs.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8f0a5617",
   "metadata": {},
   "outputs": [],
   "source": [
    "upsample_block_tf = UpSampleRatioTF(128, 2)\n",
    "tf_outputs = upsample_block_tf(dummy_inputs)\n",
    "\n",
    "np.testing.assert_allclose(tf_outputs, np.array(jax_outputs), rtol=1e-4, atol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "78dba7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CALayer(nn.Module):\n",
    "  \"\"\"Squeeze-and-excitation block for channel attention.\n",
    "  ref: https://arxiv.org/abs/1709.01507\n",
    "  \"\"\"\n",
    "  features: int\n",
    "  reduction: int = 4\n",
    "  use_bias: bool = True\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    # 2D global average pooling\n",
    "    y = jnp.mean(x, axis=[1, 2], keepdims=True)\n",
    "    # Squeeze (in Squeeze-Excitation)\n",
    "    y = Conv1x1(self.features // self.reduction, use_bias=self.use_bias, kernel_init=weight_initializer, bias_init=weight_initializer)(y)\n",
    "    y = nn.relu(y)\n",
    "    # Excitation (in Squeeze-Excitation)\n",
    "    y = Conv1x1(self.features, use_bias=self.use_bias, kernel_init=weight_initializer, bias_init=weight_initializer)(y)\n",
    "    y = nn.sigmoid(y)\n",
    "    return x * y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "41353e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CALayerTF(\n",
    "    num_channels: int,\n",
    "    reduction: int = 4,\n",
    "    use_bias: bool = True,\n",
    "    name: str = \"channel_attention\",\n",
    "):\n",
    "    \"\"\"Squeeze-and-excitation block for channel attention.\n",
    "    ref: https://arxiv.org/abs/1709.01507\n",
    "    \"\"\"\n",
    "\n",
    "    def apply(x):\n",
    "        # 2D global average pooling\n",
    "        y = layers.GlobalAvgPool2D(keepdims=True)(x)\n",
    "        # Squeeze (in Squeeze-Excitation)\n",
    "        y = Conv1x1TF(\n",
    "            filters=num_channels // reduction, use_bias=use_bias, name=f\"{name}_Conv_0\",\n",
    "            kernel_initializer=\"ones\", bias_initializer=\"ones\",\n",
    "        )(y)\n",
    "        y = tf.nn.relu(y)\n",
    "        # Excitation (in Squeeze-Excitation)\n",
    "        y = Conv1x1TF(filters=num_channels, use_bias=use_bias, name=f\"{name}_Conv_1\", kernel_initializer=\"ones\", bias_initializer=\"ones\")(y)\n",
    "        y = tf.nn.sigmoid(y)\n",
    "        return x * y\n",
    "\n",
    "    return apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ca5c18b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_block_jax = CALayer(features=32)\n",
    "variables = ca_block_jax.init(random.PRNGKey(0), dummy_inputs.numpy())\n",
    "jax_outputs = ca_block_jax.apply(variables, dummy_inputs.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0580ed62",
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_block_tf = CALayerTF(32)\n",
    "tf_outputs = ca_block_tf(dummy_inputs)\n",
    "\n",
    "np.testing.assert_allclose(tf_outputs, np.array(jax_outputs), rtol=1e-5, atol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7e44dbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RCAB(nn.Module):\n",
    "  \"\"\"Residual channel attention block. Contains LN,Conv,lRelu,Conv,SELayer.\"\"\"\n",
    "  features: int\n",
    "  reduction: int = 4\n",
    "  lrelu_slope: float = 0.2\n",
    "  use_bias: bool = True\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    shortcut = x\n",
    "    x = nn.LayerNorm(name=\"LayerNorm\")(x)\n",
    "    x = Conv3x3(features=self.features, use_bias=self.use_bias, name=\"conv1\", kernel_init=weight_initializer, bias_init=weight_initializer)(x)\n",
    "    x = nn.leaky_relu(x, negative_slope=self.lrelu_slope)\n",
    "    x = Conv3x3(features=self.features, use_bias=self.use_bias, name=\"conv2\", kernel_init=weight_initializer, bias_init=weight_initializer)(x)\n",
    "    x = CALayer(features=self.features, reduction=self.reduction,\n",
    "                use_bias=self.use_bias, name=\"channel_attention\")(x)\n",
    "    return x + shortcut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b3ca4d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "Conv3x3TF = functools.partial(layers.Conv2D, kernel_size=(3, 3), padding=\"same\")\n",
    "\n",
    "def RCABTF(\n",
    "    num_channels: int,\n",
    "    reduction: int = 4,\n",
    "    lrelu_slope: float = 0.2,\n",
    "    use_bias: bool = True,\n",
    "    name: str = \"residual_ca\",\n",
    "):\n",
    "    \"\"\"Residual channel attention block. Contains LN,Conv,lRelu,Conv,SELayer.\"\"\"\n",
    "\n",
    "    def apply(x):\n",
    "        shortcut = x\n",
    "        x = layers.LayerNormalization(epsilon=1e-06, name=f\"{name}_LayerNorm\")(x)\n",
    "        x = Conv3x3TF(filters=num_channels, use_bias=use_bias, name=f\"{name}_conv1\", kernel_initializer=\"ones\", bias_initializer=\"ones\")(x)\n",
    "        x = tf.nn.leaky_relu(x, alpha=lrelu_slope)\n",
    "        x = Conv3x3TF(filters=num_channels, use_bias=use_bias, name=f\"{name}_conv2\", kernel_initializer=\"ones\", bias_initializer=\"ones\")(x)\n",
    "        x = CALayerTF(\n",
    "            num_channels=num_channels,\n",
    "            reduction=reduction,\n",
    "            use_bias=use_bias,\n",
    "            name=f\"{name}_channel_attention\",\n",
    "        )(x)\n",
    "        return x + shortcut\n",
    "    \n",
    "    return apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e1cb70e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rcab_block_jax = RCAB(features=32)\n",
    "variables = rcab_block_jax.init(random.PRNGKey(0), dummy_inputs.numpy())\n",
    "jax_outputs = rcab_block_jax.apply(variables, dummy_inputs.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "99052cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rcab_block_tf = RCABTF(32)\n",
    "tf_outputs = rcab_block_tf(dummy_inputs)\n",
    "\n",
    "np.testing.assert_allclose(tf_outputs, np.array(jax_outputs), rtol=1e-5, atol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "542e5c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridGatingUnit(nn.Module):\n",
    "  \"\"\"A SpatialGatingUnit as defined in the gMLP paper.\n",
    "  The 'spatial' dim is defined as the second last.\n",
    "  If applied on other dims, you should swapaxes first.\n",
    "  \"\"\"\n",
    "  use_bias: bool = True\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    u, v = jnp.split(x, 2, axis=-1)\n",
    "    v = nn.LayerNorm(name=\"intermediate_layernorm\")(v)\n",
    "    n = x.shape[-3]   # get spatial dim\n",
    "    v = jnp.swapaxes(v, -1, -3)\n",
    "    v = nn.Dense(n, use_bias=self.use_bias, kernel_init=weight_initializer, bias_init=weight_initializer)(v)\n",
    "    v = jnp.swapaxes(v, -1, -3)\n",
    "    return u * (v + 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7b35361a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.experimental.numpy as tnp\n",
    "\n",
    "class SwapAxes(layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, x, axis_one, axis_two):\n",
    "        return tnp.swapaxes(x, axis_one, axis_two)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        return config\n",
    "\n",
    "\n",
    "def GridGatingUnitTF(use_bias: bool = True, name: str = \"grid_gating_unit\"):\n",
    "    \"\"\"A SpatialGatingUnit as defined in the gMLP paper.\n",
    "    The 'spatial' dim is defined as the second last.\n",
    "    If applied on other dims, you should swapaxes first.\n",
    "    \"\"\"\n",
    "\n",
    "    def apply(x):\n",
    "        u, v = tf.split(x, 2, axis=-1)\n",
    "        v = layers.LayerNormalization(\n",
    "            epsilon=1e-06, name=f\"{name}_intermediate_layernorm\"\n",
    "        )(v)\n",
    "        n = K.int_shape(x)[-3]  # get spatial dim\n",
    "        v = SwapAxes()(v, -1, -3)\n",
    "        v = layers.Dense(n, use_bias=use_bias, name=f\"{name}_Dense_0\", kernel_initializer=\"ones\", bias_initializer=\"ones\")(v)\n",
    "        v = SwapAxes()(v, -1, -3)\n",
    "        return u * (v + 1.)\n",
    "\n",
    "    return apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "17b73f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gg_block_jax = GridGatingUnit()\n",
    "variables = gg_block_jax.init(random.PRNGKey(0), dummy_inputs.numpy())\n",
    "jax_outputs = gg_block_jax.apply(variables, dummy_inputs.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8b3c8d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gg_block_tf = GridGatingUnitTF()\n",
    "tf_outputs = gg_block_tf(dummy_inputs)\n",
    "\n",
    "np.testing.assert_allclose(tf_outputs, np.array(jax_outputs), atol=1e-4, rtol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1a0e9c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridGmlpLayer(nn.Module):\n",
    "  \"\"\"Grid gMLP layer that performs global mixing of tokens.\"\"\"\n",
    "  grid_size: Sequence[int]\n",
    "  use_bias: bool = True\n",
    "  factor: int = 2\n",
    "  dropout_rate: float = 0.0\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x, deterministic=True):\n",
    "    n, h, w, num_channels = x.shape\n",
    "    gh, gw = self.grid_size\n",
    "    fh, fw = h // gh, w // gw\n",
    "    x = block_images_einops(x, patch_size=(fh, fw))\n",
    "    # gMLP1: Global (grid) mixing part, provides global grid communication.\n",
    "    y = nn.LayerNorm(name=\"LayerNorm\")(x)\n",
    "    y = nn.Dense(num_channels * self.factor, use_bias=self.use_bias,\n",
    "                 kernel_init=weight_initializer, bias_init=weight_initializer,\n",
    "                name=\"in_project\")(y)\n",
    "    y = nn.gelu(y)\n",
    "    y = GridGatingUnit(use_bias=self.use_bias, name=\"GridGatingUnit\")(y)\n",
    "    y = nn.Dense(num_channels, use_bias=self.use_bias,\n",
    "                 kernel_init=weight_initializer,\n",
    "                 bias_init=weight_initializer, name=\"out_project\")(y)\n",
    "    y = nn.Dropout(self.dropout_rate)(y, deterministic)\n",
    "    x = x + y\n",
    "    x = unblock_images_einops(x, grid_size=(gh, gw), patch_size=(fh, fw))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "03f96854",
   "metadata": {},
   "outputs": [],
   "source": [
    "from maxim.layers import BlockImages, SwapAxes, UnblockImages\n",
    "\n",
    "\n",
    "def GridGmlpLayerTF(\n",
    "    grid_size,\n",
    "    use_bias: bool = True,\n",
    "    factor: int = 2,\n",
    "    dropout_rate: float = 0.0,\n",
    "    name: str = \"grid_gmlp\",\n",
    "):\n",
    "    \"\"\"Grid gMLP layer that performs global mixing of tokens.\"\"\"\n",
    "\n",
    "    def apply(x):\n",
    "        n, h, w, num_channels = (\n",
    "            K.int_shape(x)[0],\n",
    "            K.int_shape(x)[1],\n",
    "            K.int_shape(x)[2],\n",
    "            K.int_shape(x)[3],\n",
    "        )\n",
    "        gh, gw = grid_size\n",
    "        fh, fw = h // gh, w // gw\n",
    "\n",
    "        x = BlockImages()(x, patch_size=(fh, fw))\n",
    "        # gMLP1: Global (grid) mixing part, provides global grid communication.\n",
    "        y = layers.LayerNormalization(epsilon=1e-06, name=f\"{name}_LayerNorm\")(x)\n",
    "        y = layers.Dense(\n",
    "            num_channels * factor,\n",
    "            use_bias=use_bias,\n",
    "            name=f\"{name}_in_project\",\n",
    "            kernel_initializer=\"ones\", bias_initializer=\"ones\",\n",
    "        )(y)\n",
    "        y = tf.nn.gelu(y, approximate=True)\n",
    "        y = GridGatingUnitTF(use_bias=use_bias, name=f\"{name}_GridGatingUnit\")(y)\n",
    "        y = layers.Dense(\n",
    "            num_channels,\n",
    "            use_bias=use_bias,\n",
    "            name=f\"{name}_out_project\",\n",
    "            kernel_initializer=\"ones\", bias_initializer=\"ones\",\n",
    "        )(y)\n",
    "        y = layers.Dropout(dropout_rate)(y)\n",
    "        x = x + y\n",
    "        x = UnblockImages()(x, grid_size=(gh, gw), patch_size=(fh, fw))\n",
    "        return x\n",
    "\n",
    "    return apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5a338a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmlp_jax = GridGmlpLayer(grid_size=(16, 16))\n",
    "variables = gmlp_jax.init(random.PRNGKey(0), dummy_inputs.numpy())\n",
    "jax_outputs = gmlp_jax.apply(variables, dummy_inputs.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "03d9782d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmlp_tf = GridGmlpLayerTF(grid_size=(16, 16))\n",
    "tf_outputs = gmlp_tf(dummy_inputs)\n",
    "\n",
    "np.testing.assert_allclose(tf_outputs, np.array(jax_outputs), atol=1e-5, rtol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c6f7ca48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockGatingUnit(nn.Module):\n",
    "  \"\"\"A SpatialGatingUnit as defined in the gMLP paper.\n",
    "  The 'spatial' dim is defined as the **second last**.\n",
    "  If applied on other dims, you should swapaxes first.\n",
    "  \"\"\"\n",
    "  use_bias: bool = True\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    u, v = jnp.split(x, 2, axis=-1)\n",
    "    v = nn.LayerNorm(name=\"intermediate_layernorm\")(v)\n",
    "    n = x.shape[-2]  # get spatial dim\n",
    "    v = jnp.swapaxes(v, -1, -2)\n",
    "    v = nn.Dense(n, use_bias=self.use_bias, kernel_init=weight_initializer, bias_init=weight_initializer)(v)\n",
    "    v = jnp.swapaxes(v, -1, -2)\n",
    "    return u * (v + 1.)\n",
    "\n",
    "class BlockGmlpLayer(nn.Module):\n",
    "  \"\"\"Block gMLP layer that performs local mixing of tokens.\"\"\"\n",
    "  block_size: Sequence[int]\n",
    "  use_bias: bool = True\n",
    "  factor: int = 2\n",
    "  dropout_rate: float = 0.0\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x, deterministic=True):\n",
    "    n, h, w, num_channels = x.shape\n",
    "    fh, fw = self.block_size\n",
    "    gh, gw = h // fh, w // fw\n",
    "    x = block_images_einops(x, patch_size=(fh, fw))\n",
    "    # MLP2: Local (block) mixing part, provides within-block communication.\n",
    "    y = nn.LayerNorm(name=\"LayerNorm\")(x)\n",
    "    y = nn.Dense(num_channels * self.factor, use_bias=self.use_bias,\n",
    "                 kernel_init=weight_initializer, bias_init=weight_initializer, name=\"in_project\")(y)\n",
    "    y = nn.gelu(y)\n",
    "    y = BlockGatingUnit(use_bias=self.use_bias, name=\"BlockGatingUnit\")(y)\n",
    "    y = nn.Dense(num_channels, use_bias=self.use_bias,\n",
    "                 kernel_init=weight_initializer, bias_init=weight_initializer, name=\"out_project\")(y)\n",
    "    y = nn.Dropout(self.dropout_rate)(y, deterministic)\n",
    "    x = x + y\n",
    "    x = unblock_images_einops(x, grid_size=(gh, gw), patch_size=(fh, fw))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "534ad42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BlockGatingUnitTF(use_bias: bool = True, name: str = \"block_gating_unit\"):\n",
    "    \"\"\"A SpatialGatingUnit as defined in the gMLP paper.\n",
    "    The 'spatial' dim is defined as the **second last**.\n",
    "    If applied on other dims, you should swapaxes first.\n",
    "    \"\"\"\n",
    "\n",
    "    def apply(x):\n",
    "        u, v = tf.split(x, 2, axis=-1)\n",
    "        v = layers.LayerNormalization(\n",
    "            epsilon=1e-06, name=f\"{name}_intermediate_layernorm\"\n",
    "        )(v)\n",
    "        n = K.int_shape(x)[-2]  # get spatial dim\n",
    "        v = SwapAxes()(v, -1, -2)\n",
    "        v = layers.Dense(n, use_bias=use_bias, name=f\"{name}_Dense_0\", kernel_initializer=\"ones\", bias_initializer=\"ones\")(v)\n",
    "        v = SwapAxes()(v, -1, -2)\n",
    "        return u * (v + 1.0)\n",
    "\n",
    "    return apply\n",
    "\n",
    "\n",
    "def BlockGmlpLayerTF(\n",
    "    block_size,\n",
    "    use_bias: bool = True,\n",
    "    factor: int = 2,\n",
    "    dropout_rate: float = 0.0,\n",
    "    name: str = \"block_gmlp\",\n",
    "):\n",
    "    \"\"\"Block gMLP layer that performs local mixing of tokens.\"\"\"\n",
    "\n",
    "    def apply(x):\n",
    "        n, h, w, num_channels = (\n",
    "            K.int_shape(x)[0],\n",
    "            K.int_shape(x)[1],\n",
    "            K.int_shape(x)[2],\n",
    "            K.int_shape(x)[3],\n",
    "        )\n",
    "        fh, fw = block_size\n",
    "        gh, gw = h // fh, w // fw\n",
    "        x = BlockImages()(x, patch_size=(fh, fw))\n",
    "        # MLP2: Local (block) mixing part, provides within-block communication.\n",
    "        y = layers.LayerNormalization(epsilon=1e-06, name=f\"{name}_LayerNorm\")(x)\n",
    "        y = layers.Dense(\n",
    "            num_channels * factor,\n",
    "            use_bias=use_bias,\n",
    "            name=f\"{name}_in_project\",\n",
    "            kernel_initializer=\"ones\", bias_initializer=\"ones\",\n",
    "        )(y)\n",
    "        y = tf.nn.gelu(y, approximate=True)\n",
    "        y = BlockGatingUnitTF(use_bias=use_bias, name=f\"{name}_BlockGatingUnit\")(y)\n",
    "        y = layers.Dense(\n",
    "            num_channels,\n",
    "            use_bias=use_bias,\n",
    "            name=f\"{name}_out_project\",\n",
    "            kernel_initializer=\"ones\", bias_initializer=\"ones\",\n",
    "        )(y)\n",
    "        y = layers.Dropout(dropout_rate)(y)\n",
    "        x = x + y\n",
    "        x = UnblockImages()(x, grid_size=(gh, gw), patch_size=(fh, fw))\n",
    "        return x\n",
    "\n",
    "    return apply\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "eabc3977",
   "metadata": {},
   "outputs": [],
   "source": [
    "bmlp_jax = BlockGmlpLayer(block_size=(16, 16))\n",
    "variables = bmlp_jax.init(random.PRNGKey(0), dummy_inputs.numpy())\n",
    "jax_outputs = bmlp_jax.apply(variables, dummy_inputs.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b63e086e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bmlp_tf = BlockGmlpLayerTF(block_size=(16, 16))\n",
    "tf_outputs = bmlp_tf(dummy_inputs)\n",
    "\n",
    "np.testing.assert_allclose(tf_outputs, np.array(jax_outputs), atol=1e-5, rtol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f4cb6208",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualSplitHeadMultiAxisGmlpLayer(nn.Module):\n",
    "  \"\"\"The multi-axis gated MLP block.\"\"\"\n",
    "  block_size: Sequence[int]\n",
    "  grid_size: Sequence[int]\n",
    "  block_gmlp_factor: int = 2\n",
    "  grid_gmlp_factor: int = 2\n",
    "  input_proj_factor: int = 2\n",
    "  use_bias: bool = True\n",
    "  dropout_rate: float = 0.0\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x, deterministic=True):\n",
    "    shortcut = x\n",
    "    n, h, w, num_channels = x.shape\n",
    "    x = nn.LayerNorm(name=\"LayerNorm_in\")(x)\n",
    "    x = nn.Dense(num_channels * self.input_proj_factor, use_bias=self.use_bias,\n",
    "                 kernel_init=weight_initializer, bias_init=weight_initializer, name=\"in_project\")(x)\n",
    "    x = nn.gelu(x)\n",
    "\n",
    "    u, v = jnp.split(x, 2, axis=-1)\n",
    "    \n",
    "    # GridGMLPLayer\n",
    "    u = GridGmlpLayer(\n",
    "        grid_size=self.grid_size,\n",
    "        factor=self.grid_gmlp_factor,\n",
    "        use_bias=self.use_bias,\n",
    "        dropout_rate=self.dropout_rate,\n",
    "        name=\"GridGmlpLayer\")(u, deterministic)\n",
    "\n",
    "    # BlockGMLPLayer\n",
    "    v = BlockGmlpLayer(\n",
    "        block_size=self.block_size,\n",
    "        factor=self.block_gmlp_factor,\n",
    "        use_bias=self.use_bias,\n",
    "        dropout_rate=self.dropout_rate,\n",
    "        name=\"BlockGmlpLayer\")(v, deterministic)\n",
    "\n",
    "    x = jnp.concatenate([u, v], axis=-1)\n",
    "\n",
    "    x = nn.Dense(num_channels, use_bias=self.use_bias,\n",
    "                 kernel_init=weight_initializer, bias_init=weight_initializer, name=\"out_project\")(x)\n",
    "    x = nn.Dropout(self.dropout_rate)(x, deterministic)\n",
    "    x = x + shortcut\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6d01440a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResidualSplitHeadMultiAxisGmlpLayerTF(\n",
    "    block_size,\n",
    "    grid_size,\n",
    "    block_gmlp_factor: int = 2,\n",
    "    grid_gmlp_factor: int = 2,\n",
    "    input_proj_factor: int = 2,\n",
    "    use_bias: bool = True,\n",
    "    dropout_rate: float = 0.0,\n",
    "    name: str = \"residual_split_head_maxim\",\n",
    "):\n",
    "    \"\"\"The multi-axis gated MLP block.\"\"\"\n",
    "\n",
    "    def apply(x):\n",
    "        shortcut = x\n",
    "        n, h, w, num_channels = (\n",
    "            K.int_shape(x)[0],\n",
    "            K.int_shape(x)[1],\n",
    "            K.int_shape(x)[2],\n",
    "            K.int_shape(x)[3],\n",
    "        )\n",
    "        x = layers.LayerNormalization(epsilon=1e-06, name=f\"{name}_LayerNorm_in\")(x)\n",
    "\n",
    "        x = layers.Dense(\n",
    "            int(num_channels) * input_proj_factor,\n",
    "            use_bias=use_bias,\n",
    "            name=f\"{name}_in_project\",\n",
    "            kernel_initializer=\"ones\", bias_initializer=\"ones\"\n",
    "        )(x)\n",
    "        x = tf.nn.gelu(x, approximate=True)\n",
    "\n",
    "        u, v = tf.split(x, 2, axis=-1)\n",
    "\n",
    "        # GridGMLPLayer\n",
    "        u = GridGmlpLayerTF(\n",
    "            grid_size=grid_size,\n",
    "            factor=grid_gmlp_factor,\n",
    "            use_bias=use_bias,\n",
    "            dropout_rate=dropout_rate,\n",
    "            name=f\"{name}_GridGmlpLayer\",\n",
    "        )(u)\n",
    "\n",
    "        # BlockGMLPLayer\n",
    "        v = BlockGmlpLayerTF(\n",
    "            block_size=block_size,\n",
    "            factor=block_gmlp_factor,\n",
    "            use_bias=use_bias,\n",
    "            dropout_rate=dropout_rate,\n",
    "            name=f\"{name}_BlockGmlpLayer\",\n",
    "        )(v)\n",
    "\n",
    "        x = tf.concat([u, v], axis=-1)\n",
    "\n",
    "        x = layers.Dense(\n",
    "            num_channels,\n",
    "            use_bias=use_bias,\n",
    "            name=f\"{name}_out_project\",\n",
    "            kernel_initializer=\"ones\", bias_initializer=\"ones\"\n",
    "        )(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        x = x + shortcut\n",
    "        return x\n",
    "\n",
    "    return apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "13b36d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_maxim_jax = ResidualSplitHeadMultiAxisGmlpLayer(block_size=(16, 16), grid_size=(16, 16))\n",
    "variables = r_maxim_jax.init(random.PRNGKey(0), dummy_inputs.numpy())\n",
    "jax_outputs = r_maxim_jax.apply(variables, dummy_inputs.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f7644345",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_maxim_tf = ResidualSplitHeadMultiAxisGmlpLayerTF(block_size=(16, 16), grid_size=(16, 16))\n",
    "tf_outputs = r_maxim_tf(dummy_inputs)\n",
    "\n",
    "np.testing.assert_allclose(tf_outputs, np.array(jax_outputs), atol=1e-5, rtol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f6946c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RDCAB(nn.Module):\n",
    "  \"\"\"Residual dense channel attention block. Used in Bottlenecks.\"\"\"\n",
    "  features: int\n",
    "  reduction: int = 16\n",
    "  use_bias: bool = True\n",
    "  dropout_rate: float = 0.0\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x, deterministic=True):\n",
    "    y = nn.LayerNorm(name=\"LayerNorm\")(x)\n",
    "    y = MlpBlock(\n",
    "        mlp_dim=self.features,\n",
    "        dropout_rate=self.dropout_rate,\n",
    "        use_bias=self.use_bias,\n",
    "        name=\"channel_mixing\")(\n",
    "            y, deterministic=deterministic)\n",
    "    y = CALayer(\n",
    "        features=self.features,\n",
    "        reduction=self.reduction,\n",
    "        use_bias=self.use_bias,\n",
    "        name=\"channel_attention\")(\n",
    "            y)\n",
    "    x = x + y\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "316fab2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RDCABTF(\n",
    "    num_channels: int,\n",
    "    reduction: int = 16,\n",
    "    use_bias: bool = True,\n",
    "    dropout_rate: float = 0.0,\n",
    "    name: str = \"rdcab\",\n",
    "):\n",
    "    \"\"\"Residual dense channel attention block. Used in Bottlenecks.\"\"\"\n",
    "\n",
    "    def apply(x):\n",
    "        y = layers.LayerNormalization(epsilon=1e-6, name=f\"{name}_LayerNorm\")(x)\n",
    "        y = MlpBlockTF(\n",
    "            mlp_dim=num_channels,\n",
    "            dropout_rate=dropout_rate,\n",
    "            use_bias=True,\n",
    "            name=f\"{name}_channel_mixing\",\n",
    "        )(y)\n",
    "        y = CALayerTF(\n",
    "            num_channels=num_channels,\n",
    "            reduction=reduction,\n",
    "            use_bias=True,\n",
    "            name=f\"{name}_channel_attention\",\n",
    "        )(y)\n",
    "        x = x + y\n",
    "        return x\n",
    "\n",
    "    return apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1d49566e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdcab_jax = RDCAB(32)\n",
    "variables = rdcab_jax.init(random.PRNGKey(0), dummy_inputs.numpy())\n",
    "jax_outputs = rdcab_jax.apply(variables, dummy_inputs.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ded9d6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdcab_tf = RDCABTF(32)\n",
    "tf_outputs = rdcab_tf(dummy_inputs)\n",
    "\n",
    "np.testing.assert_allclose(tf_outputs, np.array(jax_outputs), atol=1e-5, rtol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5e521f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleneckBlock(nn.Module):\n",
    "  \"\"\"The bottleneck block consisting of multi-axis gMLP block and RDCAB.\"\"\"\n",
    "  features: int\n",
    "  block_size: Sequence[int]\n",
    "  grid_size: Sequence[int]\n",
    "  num_groups: int = 1\n",
    "  block_gmlp_factor: int = 2\n",
    "  grid_gmlp_factor: int = 2\n",
    "  input_proj_factor: int = 2\n",
    "  channels_reduction: int = 4\n",
    "  dropout_rate: float = 0.0\n",
    "  use_bias: bool = True\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x, deterministic=True):\n",
    "    \"\"\"Applies the Mixer block to inputs.\"\"\"\n",
    "    assert x.ndim == 4  # Input has shape [batch, h, w, c]\n",
    "    n, h, w, num_channels = x.shape\n",
    "\n",
    "    # input projection\n",
    "    x = Conv1x1(self.features, use_bias=self.use_bias, name=\"input_proj\", kernel_init=weight_initializer, bias_init=weight_initializer)(x)\n",
    "    shortcut_long = x\n",
    "\n",
    "    for i in range(self.num_groups):\n",
    "      x = ResidualSplitHeadMultiAxisGmlpLayer(\n",
    "          grid_size=self.grid_size,\n",
    "          block_size=self.block_size,\n",
    "          grid_gmlp_factor=self.grid_gmlp_factor,\n",
    "          block_gmlp_factor=self.block_gmlp_factor,\n",
    "          input_proj_factor=self.input_proj_factor,\n",
    "          use_bias=self.use_bias,\n",
    "          dropout_rate=self.dropout_rate,\n",
    "          name=f\"SplitHeadMultiAxisGmlpLayer_{i}\")(x, deterministic)\n",
    "      # Channel-mixing part, which provides within-patch communication.\n",
    "      x = RDCAB(\n",
    "          features=self.features,\n",
    "          reduction=self.channels_reduction,\n",
    "          use_bias=self.use_bias,\n",
    "          name=f\"channel_attention_block_1_{i}\")(\n",
    "              x)\n",
    "\n",
    "    # long skip-connect\n",
    "    x = x + shortcut_long\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "34931ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BottleneckBlockTF(\n",
    "    features: int,\n",
    "    block_size,\n",
    "    grid_size,\n",
    "    num_groups: int = 1,\n",
    "    block_gmlp_factor: int = 2,\n",
    "    grid_gmlp_factor: int = 2,\n",
    "    input_proj_factor: int = 2,\n",
    "    channels_reduction: int = 4,\n",
    "    dropout_rate: float = 0.0,\n",
    "    use_bias: bool = True,\n",
    "    name: str = \"bottleneck_block\",\n",
    "):\n",
    "    \"\"\"The bottleneck block consisting of multi-axis gMLP block and RDCAB.\"\"\"\n",
    "\n",
    "    def apply(x):\n",
    "        \"\"\"Applies the Mixer block to inputs.\"\"\"\n",
    "\n",
    "        # input projection\n",
    "        x = Conv1x1TF(filters=features, use_bias=use_bias, name=f\"{name}_input_proj\", kernel_initializer=\"ones\", bias_initializer=\"ones\")(x)\n",
    "        shortcut_long = x\n",
    "\n",
    "        for i in range(num_groups):\n",
    "            x = ResidualSplitHeadMultiAxisGmlpLayerTF(\n",
    "                grid_size=grid_size,\n",
    "                block_size=block_size,\n",
    "                grid_gmlp_factor=grid_gmlp_factor,\n",
    "                block_gmlp_factor=block_gmlp_factor,\n",
    "                input_proj_factor=input_proj_factor,\n",
    "                use_bias=use_bias,\n",
    "                dropout_rate=dropout_rate,\n",
    "                name=f\"{name}_SplitHeadMultiAxisGmlpLayer_{i}\",\n",
    "            )(x)\n",
    "            # Channel-mixing part, which provides within-patch communication.\n",
    "            x = RDCABTF(\n",
    "                num_channels=features,\n",
    "                reduction=channels_reduction,\n",
    "                use_bias=use_bias,\n",
    "                name=f\"{name}_channel_attention_block_1_{i}\",\n",
    "            )(x)\n",
    "\n",
    "        # long skip-connect\n",
    "        x = x + shortcut_long\n",
    "        return x\n",
    "\n",
    "    return apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5e7f8b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_jax = BottleneckBlock(32, (16, 16), (16, 16))\n",
    "variables = bb_jax.init(random.PRNGKey(0), dummy_inputs.numpy())\n",
    "jax_outputs = bb_jax.apply(variables, dummy_inputs.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "77a6fdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_tf = BottleneckBlockTF(32, (16, 16), (16, 16))\n",
    "tf_outputs = bb_tf(dummy_inputs)\n",
    "\n",
    "np.testing.assert_allclose(tf_outputs, np.array(jax_outputs), atol=1e-5, rtol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3ef1a421",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.random.normal((1, 256, 256, 3))\n",
    "num_supervision_scales = 3\n",
    "x = inputs.numpy()\n",
    "\n",
    "n, h, w, c = x.shape  # input image shape\n",
    "\n",
    "shortcuts_jax = []\n",
    "shortcuts_jax.append(x)\n",
    "# Get multi-scale input images\n",
    "for i in range(1, num_supervision_scales):\n",
    "    image_resized = jax.image.resize(\n",
    "        x, shape=(n, h // (2**i), w // (2**i), c), method=\"nearest\")\n",
    "    shortcuts_jax.append(np.array(image_resized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "94a1ad42",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_supervision_scales = 3\n",
    "x = inputs\n",
    "\n",
    "n, h, w, c = x.shape  # input image shape\n",
    "\n",
    "shortcuts_tf = []\n",
    "shortcuts_tf.append(x)\n",
    "\n",
    "\n",
    "for i in range(1, num_supervision_scales):\n",
    "    resizing_layer = Resizing(\n",
    "        height=h // (2 ** i),\n",
    "        width=w // (2 ** i),\n",
    "        method=\"nearest\",\n",
    "        antialias=True,\n",
    "    )  # Following `jax.image.resize()`.\n",
    "    shortcuts_tf.append(resizing_layer(x).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c801076d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for shortcut_jax, shortcut_tf in zip(shortcuts_jax, shortcuts_tf):\n",
    "    assert np.allclose(shortcut_jax, shortcut_tf), f\"difference: {np.max(shortcut_jax - shortcut_tf)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3df7b0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_all_jax = []\n",
    "num_stages = 3\n",
    "features = 32\n",
    "\n",
    "for idx_stage in range(num_stages):\n",
    "    # Input convolution, get multi-scale input features\n",
    "    x_scales = []\n",
    "    for i in range(num_supervision_scales):\n",
    "        conv3x3_jax = Conv3x3(\n",
    "            (2**i) * features,\n",
    "            use_bias=True,\n",
    "            kernel_init=nn.initializers.ones,\n",
    "            bias_init=nn.initializers.ones,\n",
    "            name=f\"stage_{idx_stage}_input_conv_{i}\")\n",
    "\n",
    "        variables = conv3x3_jax.init(random.PRNGKey(0), shortcuts_jax[i])\n",
    "        x_scale = conv3x3_jax.apply(variables, shortcuts_jax[i])\n",
    "        outputs_all_jax.append(np.array(x_scale))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e91df562",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_all_tf = []\n",
    "\n",
    "for idx_stage in range(num_stages):\n",
    "    # Input convolution, get multi-scale input features\n",
    "    x_scales = []\n",
    "    for i in range(num_supervision_scales):\n",
    "        x_scale = Conv3x3TF(\n",
    "            filters=(2 ** i) * features,\n",
    "            use_bias=True,\n",
    "            name=f\"stage_{idx_stage}_input_conv_{i}\",\n",
    "            kernel_initializer=\"ones\",\n",
    "            bias_initializer=\"ones\"\n",
    "        )(shortcuts_tf[i])\n",
    "        outputs_all_tf.append(x_scale.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "10e754bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for shortcut_jax, shortcut_tf in zip(outputs_all_jax, outputs_all_tf):\n",
    "    assert np.allclose(shortcut_jax, shortcut_tf), f\"difference: {np.max(shortcut_jax - shortcut_tf)}\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 ('keras-io')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "e2d305ce7cd0384a806e1593585eb2c9440fc4f311ceabe0eabf88d87d0cc13f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
